This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

- Pay special attention to the Repository Instruction. These contain important context and guidelines specific to this project.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/
  workflows/
    claude.yml
.gitignore
auth.go
cache.go
CLAUDE.md
config_test.go
config.go
context.go
direct_handlers.go
fallback_models.go
fetch_models.go
files.go
gemini_server.go
gemini_test.go
gemini_utils.go
go.mod
handlers_common.go
http_server.go
logger.go
main.go
model_functions.go
prompt_handlers.go
prompts_test.go
prompts.go
README.md
refactoring_plan.md
run_format.sh
run_lint.sh
run_test.sh
server_handlers.go
structs.go
tools.go
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="logger.go">
package main

import (
	"fmt"
	"io"
	"os"
	"time"
)

// LogLevel represents the severity level of a log message
type LogLevel int

const (
	// LevelDebug is for detailed troubleshooting
	LevelDebug LogLevel = iota
	// LevelInfo is for general operational messages
	LevelInfo
	// LevelWarning is for potential issues
	LevelWarning
	// LevelError is for error conditions
	LevelError
)

// Logger provides a consistent logging interface
type Logger interface {
	Debug(format string, args ...interface{})
	Info(format string, args ...interface{})
	Warn(format string, args ...interface{})
	Error(format string, args ...interface{})
}

// StandardLogger implements the Logger interface
type StandardLogger struct {
	level  LogLevel
	writer io.Writer
}

// NewLogger creates a new standard logger with the specified level
func NewLogger(level LogLevel) Logger {
	return &StandardLogger{
		level:  level,
		writer: os.Stderr, // Default to stderr
	}
}

// Debug logs a debug message
func (l *StandardLogger) Debug(format string, args ...interface{}) {
	if l.level <= LevelDebug {
		l.log("DEBUG", format, args...)
	}
}

// Info logs an informational message
func (l *StandardLogger) Info(format string, args ...interface{}) {
	if l.level <= LevelInfo {
		l.log("INFO", format, args...)
	}
}

// Warn logs a warning message
func (l *StandardLogger) Warn(format string, args ...interface{}) {
	if l.level <= LevelWarning {
		l.log("WARN", format, args...)
	}
}

// Error logs an error message
func (l *StandardLogger) Error(format string, args ...interface{}) {
	if l.level <= LevelError {
		l.log("ERROR", format, args...)
	}
}

// log writes a formatted log message to the writer
func (l *StandardLogger) log(level, format string, args ...interface{}) {
	timestamp := time.Now().Format("2006-01-02 15:04:05")
	message := fmt.Sprintf(format, args...)
	fmt.Fprintf(l.writer, "[%s] %s: %s\n", timestamp, level, message)
}
</file>

<file path=".github/workflows/claude.yml">
name: Claude PR Assistant

on:
  issue_comment:
    types: [created]
  pull_request_review_comment:
    types: [created]
  issues:
    types: [opened, assigned]
  pull_request_review:
    types: [submitted]

jobs:
  claude-code-action:
    if: |
      (github.event_name == 'issue_comment' && contains(github.event.comment.body, '@claude')) ||
      (github.event_name == 'pull_request_review_comment' && contains(github.event.comment.body, '@claude')) ||
      (github.event_name == 'pull_request_review' && contains(github.event.review.body, '@claude')) ||
      (github.event_name == 'issues' && contains(github.event.issue.body, '@claude'))
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: read
      issues: read
      id-token: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Run Claude PR Action
        uses: anthropics/claude-code-action@beta
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          timeout_minutes: "60"
</file>

<file path="context.go">
package main

// contextKey is a type for context keys to prevent collisions
type contextKey string

// Context keys
const (
	// loggerKey is the context key for the logger
	loggerKey contextKey = "logger"
	// configKey is the context key for the configuration
	configKey contextKey = "config"
	// authErrorKey is the context key for authentication errors
	authErrorKey contextKey = "auth_error"
	// authenticatedKey is the context key for authentication status
	authenticatedKey contextKey = "authenticated"
	// userIDKey is the context key for user ID
	userIDKey contextKey = "user_id"
	// usernameKey is the context key for username
	usernameKey contextKey = "username"
	// userRoleKey is the context key for user role
	userRoleKey contextKey = "user_role"
	// httpMethodKey is the context key for HTTP method
	httpMethodKey contextKey = "http_method"
	// httpPathKey is the context key for HTTP path
	httpPathKey contextKey = "http_path"
	// httpRemoteAddrKey is the context key for HTTP remote address
	httpRemoteAddrKey contextKey = "http_remote_addr"
)
</file>

<file path="http_server.go">
package main

import (
	"context"
	"encoding/json"
	"fmt"
	"net/http"
	"os"
	"os/signal"
	"strings"
	"sync"
	"syscall"

	"github.com/mark3labs/mcp-go/server"
)

// startHTTPServer starts the HTTP transport server
func startHTTPServer(ctx context.Context, mcpServer *server.MCPServer, config *Config, logger Logger) error {
	// Create HTTP server options
	var opts []server.StreamableHTTPOption

	// Configure heartbeat if enabled
	if config.HTTPHeartbeat > 0 {
		opts = append(opts, server.WithHeartbeatInterval(config.HTTPHeartbeat))
	}

	// Configure stateless mode
	if config.HTTPStateless {
		opts = append(opts, server.WithStateLess(true))
	}

	// Configure endpoint path
	opts = append(opts, server.WithEndpointPath(config.HTTPPath))

	// Add HTTP context function for CORS, logging, and authentication
	if config.HTTPCORSEnabled || config.AuthEnabled {
		opts = append(opts, server.WithHTTPContextFunc(createHTTPMiddleware(config, logger)))
	}

	// Create streamable HTTP server
	httpServer := server.NewStreamableHTTPServer(mcpServer, opts...)

	// Create custom HTTP server with OAuth well-known endpoint
	customServer := &http.Server{
		Addr:    config.HTTPAddress,
		Handler: createCustomHTTPHandler(httpServer, config, logger),
	}

	// Set up graceful shutdown
	ctx, cancel := context.WithCancel(ctx)
	defer cancel()

	// Handle shutdown signals
	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)

	var wg sync.WaitGroup
	wg.Add(1)

	// Start server in goroutine
	go func() {
		defer wg.Done()
		if err := customServer.ListenAndServe(); err != nil && err != http.ErrServerClosed {
			logger.Error("HTTP server failed to start: %v", err)
			cancel()
		}
	}()

	// Wait for shutdown signal
	select {
	case sig := <-sigChan:
		logger.Info("Received signal %v, shutting down HTTP server...", sig)
	case <-ctx.Done():
		logger.Info("Context cancelled, shutting down HTTP server...")
	}

	// Graceful shutdown
	shutdownCtx, shutdownCancel := context.WithTimeout(context.Background(), config.HTTPTimeout)
	defer shutdownCancel()

	if err := customServer.Shutdown(shutdownCtx); err != nil {
		logger.Error("HTTP server shutdown error: %v", err)
		return err
	}

	wg.Wait()
	logger.Info("HTTP server stopped")
	return nil
}

// createHTTPMiddleware creates an HTTP context function with CORS, logging, and authentication
func createHTTPMiddleware(config *Config, logger Logger) server.HTTPContextFunc {
	// Create authentication middleware
	var authMiddleware *AuthMiddleware
	if config.AuthEnabled {
		authMiddleware = NewAuthMiddleware(config.AuthSecretKey, config.AuthEnabled, logger)
		logger.Info("HTTP authentication enabled")
	}

	return func(ctx context.Context, r *http.Request) context.Context {
		// Log HTTP request
		logger.Info("HTTP %s %s from %s", r.Method, r.URL.Path, r.RemoteAddr)

		// Apply authentication middleware if enabled
		if authMiddleware != nil {
			// Create a wrapper function for the next middleware step
			nextFunc := func(ctx context.Context, r *http.Request) context.Context {
				return ctx
			}
			// Apply authentication middleware
			ctx = authMiddleware.HTTPContextFunc(nextFunc)(ctx, r)
		}

		// Add CORS headers if enabled
		if config.HTTPCORSEnabled {
			// Check if request origin is allowed
			origin := r.Header.Get("Origin")
			if origin != "" && isOriginAllowed(origin, config.HTTPCORSOrigins) {
				// Note: We can't set response headers directly here as this is a context function
				// CORS headers would need to be handled at the HTTP server level
				logger.Info("CORS: Origin %s is allowed", origin)
			}
		}

		// Add request info to context
		ctx = context.WithValue(ctx, httpMethodKey, r.Method)
		ctx = context.WithValue(ctx, httpPathKey, r.URL.Path)
		ctx = context.WithValue(ctx, httpRemoteAddrKey, r.RemoteAddr)

		return ctx
	}
}

// isOriginAllowed checks if the origin is in the allowed list
func isOriginAllowed(origin string, allowedOrigins []string) bool {
	for _, allowed := range allowedOrigins {
		if allowed == "*" || allowed == origin {
			return true
		}
		// Support wildcard subdomains (e.g., "*.example.com")
		if strings.HasPrefix(allowed, "*.") {
			domain := strings.TrimPrefix(allowed, "*.")
			if strings.HasSuffix(origin, domain) {
				return true
			}
		}
	}
	return false
}

// createCustomHTTPHandler creates a custom HTTP handler that includes OAuth well-known endpoint
func createCustomHTTPHandler(mcpHandler http.Handler, config *Config, logger Logger) http.Handler {
	mux := http.NewServeMux()

	// Add OAuth well-known endpoint
	mux.HandleFunc("/.well-known/oauth-authorization-server", func(w http.ResponseWriter, r *http.Request) {
		logger.Info("OAuth well-known endpoint accessed from %s", r.RemoteAddr)

		// Create OAuth authorization server metadata
		metadata := map[string]interface{}{
			"issuer":                           fmt.Sprintf("http://%s", r.Host),
			"authorization_endpoint":           fmt.Sprintf("http://%s/oauth/authorize", r.Host),
			"token_endpoint":                   fmt.Sprintf("http://%s/oauth/token", r.Host),
			"response_types_supported":         []string{"code"},
			"grant_types_supported":            []string{"authorization_code"},
			"code_challenge_methods_supported": []string{"S256"},
		}

		w.Header().Set("Content-Type", "application/json")
		w.Header().Set("Cache-Control", "public, max-age=3600")

		// Add CORS headers if enabled
		if config.HTTPCORSEnabled {
			origin := r.Header.Get("Origin")
			if origin != "" && isOriginAllowed(origin, config.HTTPCORSOrigins) {
				w.Header().Set("Access-Control-Allow-Origin", origin)
				w.Header().Set("Access-Control-Allow-Methods", "GET, OPTIONS")
				w.Header().Set("Access-Control-Allow-Headers", "Content-Type, Authorization")
			}
		}

		if err := json.NewEncoder(w).Encode(metadata); err != nil {
			logger.Error("Failed to encode OAuth metadata: %v", err)
			http.Error(w, "Internal Server Error", http.StatusInternalServerError)
		}
	})

	// Handle all other requests with the MCP handler
	mux.Handle("/", mcpHandler)

	return mux
}
</file>

<file path="refactoring_plan.md">
### Proposed Refactoring Plan

Here is a detailed, step-by-step plan to implement the new, more efficient architecture.

---

#### **Phase 1: Redefine the Prompt Handler's Role**

The goal of this phase is to make prompt handlers act as "template generators," not file processors.

1.  **Modify `prompt_handlers.go` - Refactor `handlePrompt`:**
    *   The function will no longer call `readLocalFiles`. Its responsibility will end at expanding file paths.
    *   The `PromptBuilder` function signature will be changed to `func(req mcp.GetPromptRequest, language string) (string, string)`, removing the `codeContent` parameter.
    *   The function will now return a structured JSON object containing the `system_prompt`, `user_prompt_template`, and the list of `file_paths`. This JSON will be returned in the `Description` field of the `mcp.GetPromptResult`. The `Messages` field will be empty.

2.  **Modify `prompt_handlers.go` - Update All Prompt Handlers:**
    *   Each handler (e.g., `CodeReviewHandler`, `DocGenerateHandler`) will be updated to use the new `handlePrompt`.
    *   The `PromptBuilder` they provide will now generate a `user_prompt_template` with a placeholder (e.g., `Please review the following code:

{{file_content}}`) instead of the actual code.

---

#### **Phase 2: Clean Up Utilities and Configuration**

This phase removes the now-unnecessary code.

1.  **Modify `gemini_utils.go`:**
    *   Remove the `readLocalFiles`, `detectLanguageFromPath`, and `detectPrimaryLanguage` functions entirely.
    *   The `ProjectLanguage` setting from the config will now be passed directly to the `PromptBuilder` in `handlePrompt`.

2.  **Modify `config.go` and `structs.go`:**
    *   The `ProjectLanguage` configuration will be kept as it's still useful for the templates. No other changes are needed here.

---

#### **Phase 3: Empower the `gemini_ask` Tool**

This phase makes the `gemini_ask` tool flexible enough to handle the new workflow.

1.  **Modify `direct_handlers.go` - Update `GeminiAskHandler`:**
    *   I will adjust the logic for handling the `systemPrompt` parameter.
    *   The new logic will be:
        1.  Check if the `systemPrompt` argument exists in the request.
        2.  If it exists (even if it's an empty string), use the value provided by the client.
        3.  If the argument does **not** exist, then and only then fall back to the default `config.GeminiSystemPrompt`.
    *   This change is critical as it allows the client to suppress the default system prompt, preventing the "double prompt" issue.

---

#### **Phase 4: Update Tests**

The tests must be updated to reflect the new architecture.

1.  **Modify `prompts_test.go`:**
    *   The tests will be rewritten to validate the new JSON output from the prompt handlers.
    *   Instead of checking for file content in the result, the tests will parse the JSON from the `Description` field and assert that the `system_prompt`, `user_prompt_template`, and `file_paths` are correct.
    *   Tests for file I/O within the prompt handlers will be removed.
</file>

<file path="run_format.sh">
#!/bin/sh

export PATH="/usr/local/go/bin:$PATH"

# Run gofmt to format all Go files recursively
/usr/local/go/bin/gofmt -w .
</file>

<file path="run_lint.sh">
#!/bin/sh

export PATH="/usr/local/go/bin:$PATH"
export HOME="/Users/rrj"
export GOLANGCI_LINT_CACHE="$HOME/Library/Caches/golangci-lint"
export GOCACHE="$HOME/.cache/go-build"

# Run linter
/Users/rrj/Projekty/Go/bin/golangci-lint run --fix ./...
</file>

<file path="run_test.sh">
#!/bin/sh

export PATH="/usr/local/go/bin:$PATH"

# Run tests
go test -v ./...
</file>

<file path="server_handlers.go">
package main

import (
	"context"
	"fmt"
	"os"

	"github.com/mark3labs/mcp-go/mcp"
	"github.com/mark3labs/mcp-go/server"
)

// setupGeminiServer creates and registers Gemini server tools
func setupGeminiServer(ctx context.Context, mcpServer *server.MCPServer, config *Config) error {
	loggerValue := ctx.Value(loggerKey)
	logger, ok := loggerValue.(Logger)
	if !ok {
		return fmt.Errorf("logger not found in context")
	}

	// Create the Gemini service with configuration
	geminiSvc, err := NewGeminiServer(ctx, config)
	if err != nil {
		return fmt.Errorf("failed to create Gemini service: %w", err)
	}

	// Create handler for gemini_ask using direct handler
	// Register gemini_ask with logger wrapper using shared tool definition
	mcpServer.AddTool(GeminiAskTool, wrapHandlerWithLogger(geminiSvc.GeminiAskHandler, "gemini_ask", logger))
	logger.Info("Registered tool: gemini_ask")

	// Use shared tool definition for gemini_search

	// Create handler for gemini_search using direct handler
	// Register gemini_search with logger wrapper using shared tool definition
	mcpServer.AddTool(GeminiSearchTool, wrapHandlerWithLogger(geminiSvc.GeminiSearchHandler, "gemini_search", logger))
	logger.Info("Registered tool: gemini_search")

	// Use shared tool definition for gemini_models

	// Create handler for gemini_models using direct handler
	// Register gemini_models with logger wrapper using shared tool definition
	mcpServer.AddTool(GeminiModelsTool, wrapHandlerWithLogger(geminiSvc.GeminiModelsHandler, "gemini_models", logger))
	logger.Info("Registered tool: gemini_models")

	// Register all prompts from the definitions
	for _, p := range Prompts {
		handler := geminiSvc.promptHandler(p)
		mcpServer.AddPrompt(*p.Prompt, wrapPromptHandlerWithLogger(handler, p.Name, logger))
		logger.Info("Registered prompt: %s", p.Name)
	}

	// Log file handling configuration
	logger.Info("File handling: max size %s, allowed types: %v",
		humanReadableSize(config.MaxFileSize),
		config.AllowedFileTypes)

	// Log cache configuration if enabled
	if config.EnableCaching {
		logger.Info("Cache settings: default TTL %v", config.DefaultCacheTTL)
	}

	// Log thinking configuration if enabled
	model := GetModelByID(config.GeminiModel)
	if config.EnableThinking && model != nil && model.SupportsThinking {
		logger.Info("Thinking mode enabled for model %s with context window size %d tokens",
			config.GeminiModel, model.ContextWindowSize)
	}

	// Log a truncated version of the system prompt for security/brevity
	promptPreview := config.GeminiSystemPrompt
	if len(promptPreview) > 50 {
		// Use proper UTF-8 safe truncation
		runeCount := 0
		for i := range promptPreview {
			runeCount++
			if runeCount > 50 {
				promptPreview = promptPreview[:i] + "..."
				break
			}
		}
	}
	logger.Info("Using system prompt: %s", promptPreview)

	return nil
}

// handleStartupError handles initialization errors by setting up an error server
func handleStartupError(ctx context.Context, err error) {
	// Safely extract logger from context
	loggerValue := ctx.Value(loggerKey)
	logger, ok := loggerValue.(Logger)
	if !ok {
		// Fallback to a new logger if type assertion fails
		logger = NewLogger(LevelError)
	}
	errorMsg := err.Error()

	logger.Error("Initialization error: %v", err)

	// Get config for EnableCaching status (if available)
	var config *Config
	configValue := ctx.Value(configKey)
	if configValue != nil {
		if cfg, ok := configValue.(Config); ok {
			config = &cfg
		}
	}

	// Create MCP server in degraded mode
	mcpServer := server.NewMCPServer(
		"gemini",
		"1.0.0",
	)

	// Create error server
	errorServer := &ErrorGeminiServer{
		errorMessage: errorMsg,
		config:       config,
	}

	// Register error handling for tools
	registerErrorTools(mcpServer, errorServer, logger)

	// Start server in degraded mode
	logger.Info("Starting Gemini MCP server in degraded mode")
	if err := server.ServeStdio(mcpServer); err != nil {
		logger.Error("Server error in degraded mode: %v", err)
		os.Exit(1)
	}
}

// Define the expected handler signature for tools
type MCPToolHandlerFunc = server.ToolHandlerFunc

// wrapHandlerWithLogger creates a middleware wrapper for logging and authentication around a tool handler
func wrapHandlerWithLogger(handler server.ToolHandlerFunc, toolName string, logger Logger) server.ToolHandlerFunc {
	return func(ctx context.Context, req mcp.CallToolRequest) (*mcp.CallToolResult, error) {
		logger.Info("Calling tool '%s'...", toolName)

		// Check authentication for HTTP requests if enabled
		// Note: We need to check if this is an HTTP request and if auth is enabled
		if httpMethod, ok := ctx.Value(httpMethodKey).(string); ok && httpMethod != "" {
			// This is an HTTP request, check if auth is required
			// Get config from the context (we'll need to pass it through)
			if authError := getAuthError(ctx); authError != "" {
				logger.Warn("Authentication failed for tool '%s': %s", toolName, authError)
				return createErrorResult(fmt.Sprintf("Authentication required: %s", authError)), nil
			}

			// Log successful authentication if present
			if isAuthenticated(ctx) {
				userID, username, role := getUserInfo(ctx)
				logger.Info("Tool '%s' called by authenticated user %s (%s) with role %s", toolName, username, userID, role)
			}
		}

		// Call the actual handler
		resp, err := handler(ctx, req)

		if err != nil {
			logger.Error("Tool '%s' failed: %v", toolName, err)
		} else {
			logger.Info("Tool '%s' completed successfully", toolName)
		}

		// Return the original response and error
		return resp, err
	}
}

// wrapPromptHandlerWithLogger creates a middleware wrapper for logging and authentication around a prompt handler
func wrapPromptHandlerWithLogger(handler server.PromptHandlerFunc, promptName string, logger Logger) server.PromptHandlerFunc {
	return func(ctx context.Context, req mcp.GetPromptRequest) (*mcp.GetPromptResult, error) {
		logger.Info("Calling prompt '%s'...", promptName)

		// Check authentication for HTTP requests if enabled
		if httpMethod, ok := ctx.Value(httpMethodKey).(string); ok && httpMethod != "" {
			// This is an HTTP request, check if auth is required
			if authError := getAuthError(ctx); authError != "" {
				logger.Warn("Authentication failed for prompt '%s': %s", promptName, authError)
				return &mcp.GetPromptResult{
					Description: fmt.Sprintf("Authentication required: %s", authError),
					Messages:    []mcp.PromptMessage{},
				}, nil
			}

			// Log successful authentication if present
			if isAuthenticated(ctx) {
				userID, username, role := getUserInfo(ctx)
				logger.Info("Prompt '%s' called by authenticated user %s (%s) with role %s", promptName, username, userID, role)
			}
		}

		// Call the actual handler
		resp, err := handler(ctx, req)

		if err != nil {
			logger.Error("Prompt '%s' failed: %v", promptName, err)
		} else {
			logger.Info("Prompt '%s' completed successfully", promptName)
		}

		// Return the original response and error
		return resp, err
	}
}

// Register error handlers for all tools
func registerErrorTools(mcpServer *server.MCPServer, errorServer *ErrorGeminiServer, logger Logger) {
	// Register error handlers for all tools using shared tool definitions
	mcpServer.AddTool(GeminiAskTool, wrapHandlerWithLogger(errorServer.handleErrorResponse, "gemini_ask", logger))
	mcpServer.AddTool(GeminiSearchTool, wrapHandlerWithLogger(errorServer.handleErrorResponse, "gemini_search", logger))
	mcpServer.AddTool(GeminiModelsTool, wrapHandlerWithLogger(errorServer.handleErrorResponse, "gemini_models", logger))

	logger.Info("Registered error handlers for all tools")
}
</file>

<file path="tools.go">
package main

import "github.com/mark3labs/mcp-go/mcp"

// GeminiAskTool defines the gemini_ask tool specification
var GeminiAskTool = mcp.NewTool(
	"gemini_ask",
	mcp.WithDescription("Use Google's Gemini AI model to ask about complex coding problems"),
	mcp.WithString("query", mcp.Required(), mcp.Description("The coding problem that we are asking Gemini AI to work on [question + code]")),
	mcp.WithString("model", mcp.Description("Optional: Specific Gemini model to use (overrides default configuration)")),
	mcp.WithString("systemPrompt", mcp.Description("Optional: Custom system prompt to use for this request (overrides default configuration)")),
	mcp.WithArray("file_paths", mcp.Description("Optional: Paths to files to include in the request context")),
	mcp.WithBoolean("use_cache", mcp.Description("Optional: Whether to try using a cache for this request (only works with compatible models)")),
	mcp.WithString("cache_ttl", mcp.Description("Optional: TTL for cache if created (e.g., '10m', '1h'). Default is 10 minutes")),
	mcp.WithBoolean("enable_thinking", mcp.Description("Optional: Enable thinking mode to see model's reasoning process (only works with Pro models)")),
	mcp.WithNumber("thinking_budget", mcp.Description("Optional: Maximum number of tokens to allocate for the model's thinking process (0-24576)")),
	mcp.WithString("thinking_budget_level", mcp.Description("Optional: Predefined thinking budget level (none, low, medium, high)")),
	mcp.WithNumber("max_tokens", mcp.Description("Optional: Maximum token limit for the response. Default is determined by the model")),
)

// GeminiSearchTool defines the gemini_search tool specification
var GeminiSearchTool = mcp.NewTool(
	"gemini_search",
	mcp.WithDescription("Use Google's Gemini AI model with Google Search to answer questions with grounded information"),
	mcp.WithString("query", mcp.Required(), mcp.Description("The question to ask Gemini using Google Search for grounding")),
	mcp.WithString("systemPrompt", mcp.Description("Optional: Custom system prompt to use for this request (overrides default configuration)")),
	mcp.WithBoolean("enable_thinking", mcp.Description("Optional: Enable thinking mode to see model's reasoning process (when supported)")),
	mcp.WithNumber("thinking_budget", mcp.Description("Optional: Maximum number of tokens to allocate for the model's thinking process (0-24576)")),
	mcp.WithString("thinking_budget_level", mcp.Description("Optional: Predefined thinking budget level (none, low, medium, high)")),
	mcp.WithNumber("max_tokens", mcp.Description("Optional: Maximum token limit for the response. Default is determined by the model")),
	mcp.WithString("model", mcp.Description("Optional: Specific Gemini model to use (overrides default configuration)")),
	mcp.WithString("start_time", mcp.Description("Optional: Filter search results to those published after this time (RFC3339 format, e.g. '2024-01-01T00:00:00Z'). If provided, end_time must also be provided.")),
	mcp.WithString("end_time", mcp.Description("Optional: Filter search results to those published before this time (RFC3339 format, e.g. '2024-12-31T23:59:59Z'). If provided, start_time must also be provided.")),
)

// GeminiModelsTool defines the gemini_models tool specification
var GeminiModelsTool = mcp.NewTool(
	"gemini_models",
	mcp.WithDescription("List available Gemini models with descriptions"),
)
</file>

<file path="cache.go">
package main

import (
	"context"
	"errors"
	"fmt"
	"strings"
	"time"

	"google.golang.org/genai"
)

// CacheRequest struct definition moved to structs.go
// CacheInfo struct definition moved to structs.go
// CacheStore struct definition moved to structs.go

// NewCacheStore creates a new cache store
func NewCacheStore(client *genai.Client, config *Config, fileStore *FileStore) *CacheStore {
	return &CacheStore{
		client:    client,
		config:    config,
		fileStore: fileStore,
		cacheInfo: make(map[string]*CacheInfo),
	}
}

// CreateCache creates a cached context
func (cs *CacheStore) CreateCache(ctx context.Context, req *CacheRequest) (*CacheInfo, error) {
	logger := getLoggerFromContext(ctx)

	// Check if caching is enabled
	if !cs.config.EnableCaching {
		return nil, errors.New("caching is disabled")
	}

	// Input validation
	if req.Model == "" {
		return nil, errors.New("model is required")
	}

	// Validate the model
	if err := ValidateModelID(req.Model); err != nil {
		return nil, fmt.Errorf("invalid model: %w", err)
	}

	// Parse TTL
	var ttl time.Duration
	if req.TTL == "" {
		ttl = cs.config.DefaultCacheTTL
	} else {
		var err error
		ttl, err = time.ParseDuration(req.TTL)
		if err != nil {
			return nil, fmt.Errorf("invalid TTL format: %w", err)
		}
	}

	// Create config with TTL
	config := &genai.CreateCachedContentConfig{
		TTL: ttl,
	}

	// Set up display name if provided
	if req.DisplayName != "" {
		config.DisplayName = req.DisplayName
	}

	// Set up system instruction if provided
	if req.SystemPrompt != "" {
		config.SystemInstruction = genai.NewContentFromText(req.SystemPrompt, "")
	}

	// Build contents with files and text
	contents := []*genai.Content{}

	// Add files if provided
	if len(req.FileIDs) > 0 {
		logger.Info("Adding %d files to cache context", len(req.FileIDs))
		for _, fileID := range req.FileIDs {
			// Get file info
			fileInfo, err := cs.fileStore.GetFile(ctx, fileID)
			if err != nil {
				logger.Error("Failed to get file with ID %s: %v", fileID, err)
				return nil, fmt.Errorf("failed to get file with ID %s: %w", fileID, err)
			}

			// Add file to contents
			logger.Info("Adding file %s with URI %s to cache context", fileID, fileInfo.URI)
			logger.Debug("File details: Name=%s, MimeType=%s, Size=%d", fileInfo.DisplayName, fileInfo.MimeType, fileInfo.Size)
			contents = append(contents, genai.NewContentFromURI(fileInfo.URI, fileInfo.MimeType, genai.RoleUser))
		}
	}

	// Add text content if provided
	if req.Content != "" {
		logger.Debug("Adding text content to cache context")
		contents = append(contents, genai.NewContentFromText(req.Content, genai.RoleUser))
	}

	// Add contents to config if we have any
	if len(contents) > 0 {
		config.Contents = contents
	}

	// Create the cached content
	logger.Info("Creating cached content with model %s", req.Model)
	cc, err := cs.client.Caches.Create(ctx, req.Model, config)
	if err != nil {
		logger.Error("Failed to create cached content: %v", err)
		return nil, fmt.Errorf("failed to create cached content: %w", err)
	}

	// Extract ID from name (format: "cachedContents/abc123")
	id := cc.Name
	if strings.HasPrefix(cc.Name, "cachedContents/") {
		id = strings.TrimPrefix(cc.Name, "cachedContents/")
	}

	// Calculate expiration time
	expiresAt := cc.ExpireTime

	// Create cache info
	cacheInfo := &CacheInfo{
		ID:          id,
		Name:        cc.Name,
		DisplayName: cc.DisplayName,
		Model:       cc.Model,
		CreatedAt:   cc.CreateTime,
		ExpiresAt:   expiresAt,
		FileIDs:     req.FileIDs,
	}

	// Store cache info
	cs.mu.Lock()
	cs.cacheInfo[id] = cacheInfo
	cs.mu.Unlock()

	logger.Info("Cache created successfully with ID: %s", id)
	return cacheInfo, nil
}

// GetCache gets cache information by ID
func (cs *CacheStore) GetCache(ctx context.Context, id string) (*CacheInfo, error) {
	logger := getLoggerFromContext(ctx)

	// Check cache first
	cs.mu.RLock()
	info, ok := cs.cacheInfo[id]
	cs.mu.RUnlock()

	if ok {
		logger.Debug("Cache info for %s found in local cache", id)
		return info, nil
	}

	// If not in cache, try to get from API
	name := id
	if !strings.HasPrefix(id, "cachedContents/") {
		name = "cachedContents/" + id
	}

	logger.Info("Fetching cache info for %s from API", name)
	cc, err := cs.client.Caches.Get(ctx, name, nil)
	if err != nil {
		logger.Error("Failed to get cached content: %v", err)
		return nil, fmt.Errorf("failed to get cached content: %w", err)
	}

	// Extract ID from name
	cacheID := cc.Name
	if strings.HasPrefix(cc.Name, "cachedContents/") {
		cacheID = strings.TrimPrefix(cc.Name, "cachedContents/")
	}

	// Get expiration time
	expiresAt := cc.ExpireTime

	// Create cache info
	cacheInfo := &CacheInfo{
		ID:          cacheID,
		Name:        cc.Name,
		DisplayName: cc.DisplayName,
		Model:       cc.Model,
		CreatedAt:   cc.CreateTime,
		ExpiresAt:   expiresAt,
		// Note: We can't get file IDs from the API, so this will be empty
	}

	// Store in cache
	cs.mu.Lock()
	cs.cacheInfo[cacheID] = cacheInfo
	cs.mu.Unlock()

	logger.Debug("Added cache info for %s to local cache", cacheID)
	return cacheInfo, nil
}
</file>

<file path="gemini_test.go">
//go:build ignore
// +build ignore

package main

import (
	"context"
	"testing"

	"github.com/mark3labs/mcp-go/mcp"
	"google.golang.org/genai"
)

// TestNewGeminiServer tests the creation of a new GeminiServer
func TestNewGeminiServer(t *testing.T) {
	tests := []struct {
		name        string
		config      *Config
		expectError bool
	}{
		{
			name:        "nil config",
			config:      nil,
			expectError: true,
		},
		{
			name: "empty API key",
			config: &Config{
				GeminiAPIKey: "",
				GeminiModel:  "gemini-pro",
			},
			expectError: true,
		},
		// Note: We can't easily test successful creation without mocking the genai.NewClient constructor
	}

	for _, tc := range tests {
		t.Run(tc.name, func(t *testing.T) {
			_, err := NewGeminiServer(context.Background(), tc.config)
			if tc.expectError && err == nil {
				t.Errorf("expected error but got none")
			}
			if !tc.expectError && err != nil {
				t.Errorf("did not expect error but got: %v", err)
			}
		})
	}
}

// TestGeminiServerListTools tests the ListTools method of GeminiServer
func TestGeminiServerListTools(t *testing.T) {
	// Create a mock GeminiServer
	server := &GeminiServer{
		config: &Config{
			GeminiAPIKey: "test-key",
			GeminiModel:  "gemini-pro",
		},
		// We don't need a real client for this test
	}

	// Test ListTools
	resp, err := server.ListTools(context.Background())
	if err != nil {
		t.Fatalf("unexpected error: %v", err)
	}

	// Verify response
	if len(resp) != 3 {
		t.Errorf("expected 3 tools, got %d", len(resp))
	}
	if resp[0].Name != "gemini_ask" {
		t.Errorf("expected tool name 'gemini_ask', got '%s'", resp[0].Name)
	}
	if resp[1].Name != "gemini_search" {
		t.Errorf("expected tool name 'gemini_search', got '%s'", resp[1].Name)
	}
	if resp[2].Name != "gemini_models" {
		t.Errorf("expected tool name 'gemini_models', got '%s'", resp[2].Name)
	}
}

// TestErrorGeminiServerListTools tests the ListTools method of ErrorGeminiServer
func TestErrorGeminiServerListTools(t *testing.T) {
	// Create an ErrorGeminiServer
	server := &ErrorGeminiServer{
		errorMessage: "test error",
	}

	// Test ListTools
	resp, err := server.ListTools(context.Background())
	if err != nil {
		t.Fatalf("unexpected error: %v", err)
	}

	// Verify response
	if len(resp) != 3 {
		t.Errorf("expected 3 tools, got %d", len(resp))
	}
	if resp[0].Name != "gemini_ask" {
		t.Errorf("expected tool name 'gemini_ask', got '%s'", resp[0].Name)
	}
	if resp[1].Name != "gemini_search" {
		t.Errorf("expected tool name 'gemini_search', got '%s'", resp[1].Name)
	}
	if resp[2].Name != "gemini_models" {
		t.Errorf("expected tool name 'gemini_models', got '%s'", resp[2].Name)
	}
}

// TestErrorGeminiServerCallTool tests the CallTool method of ErrorGeminiServer
func TestErrorGeminiServerCallTool(t *testing.T) {
	// Create an ErrorGeminiServer
	errorMsg := "initialization failed"
	server := &ErrorGeminiServer{
		errorMessage: errorMsg,
	}

	// Test CallTool
	req := mcp.CallToolRequest{}
	req.Params.Name = "gemini_ask"
	req.Params.Arguments = map[string]interface{}{
		"query": "test query",
	}

	resp, err := server.CallTool(context.Background(), req)
	if err != nil {
		t.Fatalf("unexpected error: %v", err)
	}

	// Verify response
	if !resp.IsError {
		t.Error("expected IsError to be true")
	}
	if len(resp.Content) != 1 {
		t.Fatalf("expected 1 content item, got %d", len(resp.Content))
	}
	content, ok := resp.Content[0].(mcp.TextContent)
	if !ok {
		t.Fatalf("expected content type TextContent, got %T", resp.Content[0])
	}
	if content.Text != errorMsg {
		t.Errorf("expected error message '%s', got '%s'", errorMsg, content.Text)
	}
}

// MockGenerateContentResponse creates a mock Gemini API response for testing
func MockGenerateContentResponse(content string) *genai.GenerateContentResponse {
	return &genai.GenerateContentResponse{
		Candidates: []*genai.Candidate{
			{
				Content: &genai.Content{
					Parts: []*genai.Part{
						{Text: content},
					},
					Role: genai.RoleModel,
				},
			},
		},
	}
}

// TestFormatMCPResponse tests the formatMCPResponse method of GeminiServer
func TestFormatMCPResponse(t *testing.T) {
	// Create a GeminiServer
	server := &GeminiServer{
		config: &Config{
			GeminiAPIKey: "test-key",
			GeminiModel:  "gemini-pro",
		},
	}

	// Create a mock response
	mockContent := "This is a test response from Gemini."
	mockResp := MockGenerateContentResponse(mockContent)

	// Test formatMCPResponse
	resp := server.formatMCPResponse(mockResp)

	// Verify response
	if len(resp.Content) != 1 {
		t.Fatalf("expected 1 content item, got %d", len(resp.Content))
	}
	content, ok := resp.Content[0].(mcp.TextContent)
	if !ok {
		t.Fatalf("expected content type TextContent, got %T", resp.Content[0])
	}
	if content.Text != mockContent {
		t.Errorf("expected content '%s', got '%s'", mockContent, content.Text)
	}
}

// TestGeminiServerCallTool_InvalidTool tests CallTool with an invalid tool name
func TestGeminiServerCallTool_InvalidTool(t *testing.T) {
	// Create a GeminiServer
	server := &GeminiServer{
		config: &Config{
			GeminiAPIKey: "test-key",
			GeminiModel:  "gemini-pro",
		},
	}

	// Test with invalid tool name
	req := mcp.CallToolRequest{}
	req.Params.Name = "invalid_tool"
	req.Params.Arguments = map[string]interface{}{
		"query": "test query",
	}

	resp, err := server.CallTool(context.Background(), req)
	if err != nil {
		t.Fatalf("unexpected error: %v", err)
	}

	// Verify response
	if !resp.IsError {
		t.Error("expected IsError to be true")
	}
	if len(resp.Content) != 1 {
		t.Fatalf("expected 1 content item, got %d", len(resp.Content))
	}
	content, ok := resp.Content[0].(mcp.TextContent)
	if !ok {
		t.Fatalf("expected content type TextContent, got %T", resp.Content[0])
	}
	if content.Text != "unknown tool: invalid_tool" {
		t.Errorf("expected error message 'unknown tool: invalid_tool', got '%s'", content.Text)
	}
}

// TestGeminiServerCallTool_InvalidArgument tests CallTool with an invalid argument
func TestGeminiServerCallTool_InvalidArgument(t *testing.T) {
	// Create a GeminiServer
	server := &GeminiServer{
		config: &Config{
			GeminiAPIKey: "test-key",
			GeminiModel:  "gemini-pro",
		},
	}

	// Test with invalid argument
	req := mcp.CallToolRequest{}
	req.Params.Name = "gemini_ask"
	req.Params.Arguments = map[string]interface{}{
		"query": 123, // Not a string
	}

	resp, err := server.CallTool(context.Background(), req)
	if err != nil {
		t.Fatalf("unexpected error: %v", err)
	}

	// Verify response
	if !resp.IsError {
		t.Error("expected IsError to be true")
	}
	if len(resp.Content) != 1 {
		t.Fatalf("expected 1 content item, got %d", len(resp.Content))
	}
	content, ok := resp.Content[0].(mcp.TextContent)
	if !ok {
		t.Fatalf("expected content type TextContent, got %T", resp.Content[0])
	}
	if content.Text != "query must be a string" {
		t.Errorf("expected error message 'query must be a string', got '%s'", content.Text)
	}
}
</file>

<file path="auth.go">
package main

import (
	"context"
	"crypto/hmac"
	"crypto/sha256"
	"encoding/base64"
	"encoding/json"
	"fmt"
	"net/http"
	"os"
	"strings"
	"time"
)

// AuthMiddleware handles JWT-based authentication for HTTP transport
type AuthMiddleware struct {
	secretKey []byte
	enabled   bool
	logger    Logger
}

// Claims represents JWT token claims
type Claims struct {
	UserID    string `json:"user_id"`
	Username  string `json:"username"`
	Role      string `json:"role"`
	IssuedAt  int64  `json:"iat"`
	ExpiresAt int64  `json:"exp"`
}

// NewAuthMiddleware creates a new authentication middleware
func NewAuthMiddleware(secretKey string, enabled bool, logger Logger) *AuthMiddleware {
	return &AuthMiddleware{
		secretKey: []byte(secretKey),
		enabled:   enabled,
		logger:    logger,
	}
}

// HTTPContextFunc returns a middleware function compatible with mcp-go
func (a *AuthMiddleware) HTTPContextFunc(next func(ctx context.Context, r *http.Request) context.Context) func(ctx context.Context, r *http.Request) context.Context {
	return func(ctx context.Context, r *http.Request) context.Context {
		// If authentication is disabled, just call the next middleware
		if !a.enabled {
			return next(ctx, r)
		}

		// Extract token from Authorization header
		authHeader := r.Header.Get("Authorization")
		if !strings.HasPrefix(authHeader, "Bearer ") {
			a.logger.Warn("Missing or invalid authorization header from %s", r.RemoteAddr)
			// Set authentication error in context instead of failing the request
			ctx = context.WithValue(ctx, authErrorKey, "missing_token")
			return next(ctx, r)
		}

		token := strings.TrimPrefix(authHeader, "Bearer ")

		// Validate JWT token
		claims, err := a.validateJWT(token)
		if err != nil {
			a.logger.Warn("Invalid token from %s: %v", r.RemoteAddr, err)
			ctx = context.WithValue(ctx, authErrorKey, "invalid_token")
			return next(ctx, r)
		}

		// Check if token is expired
		if time.Now().Unix() > claims.ExpiresAt {
			a.logger.Warn("Expired token from %s", r.RemoteAddr)
			ctx = context.WithValue(ctx, authErrorKey, "expired_token")
			return next(ctx, r)
		}

		a.logger.Info("Authenticated user %s (%s) from %s", claims.Username, claims.Role, r.RemoteAddr)

		// Add user to request context
		ctx = context.WithValue(ctx, authenticatedKey, true)
		ctx = context.WithValue(ctx, userIDKey, claims.UserID)
		ctx = context.WithValue(ctx, usernameKey, claims.Username)
		ctx = context.WithValue(ctx, userRoleKey, claims.Role)

		return next(ctx, r)
	}
}

// validateJWT validates a JWT token and returns the claims
func (a *AuthMiddleware) validateJWT(tokenString string) (*Claims, error) {
	// Split the token into header, payload, and signature
	parts := strings.Split(tokenString, ".")
	if len(parts) != 3 {
		return nil, fmt.Errorf("invalid token format")
	}

	// Decode and parse header
	headerData, err := base64.RawURLEncoding.DecodeString(parts[0])
	if err != nil {
		return nil, fmt.Errorf("invalid header encoding: %w", err)
	}

	var header struct {
		Alg string `json:"alg"`
		Typ string `json:"typ"`
	}
	if err := json.Unmarshal(headerData, &header); err != nil {
		return nil, fmt.Errorf("invalid header format: %w", err)
	}

	// Verify algorithm
	if header.Alg != "HS256" {
		return nil, fmt.Errorf("unsupported algorithm: %s", header.Alg)
	}

	// Decode and parse payload
	payloadData, err := base64.RawURLEncoding.DecodeString(parts[1])
	if err != nil {
		return nil, fmt.Errorf("invalid payload encoding: %w", err)
	}

	var claims Claims
	if err := json.Unmarshal(payloadData, &claims); err != nil {
		return nil, fmt.Errorf("invalid payload format: %w", err)
	}

	// Verify signature
	expectedSignature := a.generateSignature(parts[0] + "." + parts[1])
	actualSignature, err := base64.RawURLEncoding.DecodeString(parts[2])
	if err != nil {
		return nil, fmt.Errorf("invalid signature encoding: %w", err)
	}

	if !hmac.Equal(expectedSignature, actualSignature) {
		return nil, fmt.Errorf("invalid signature")
	}

	return &claims, nil
}

// generateSignature generates HMAC-SHA256 signature for the given data
func (a *AuthMiddleware) generateSignature(data string) []byte {
	h := hmac.New(sha256.New, a.secretKey)
	h.Write([]byte(data))
	return h.Sum(nil)
}

// GenerateToken generates a JWT token for a user (utility function for testing/setup)
func (a *AuthMiddleware) GenerateToken(userID, username, role string, expirationHours int) (string, error) {
	now := time.Now()
	claims := Claims{
		UserID:    userID,
		Username:  username,
		Role:      role,
		IssuedAt:  now.Unix(),
		ExpiresAt: now.Add(time.Duration(expirationHours) * time.Hour).Unix(),
	}

	// Create header
	header := map[string]string{
		"alg": "HS256",
		"typ": "JWT",
	}

	headerBytes, err := json.Marshal(header)
	if err != nil {
		return "", fmt.Errorf("failed to marshal header: %w", err)
	}

	payloadBytes, err := json.Marshal(claims)
	if err != nil {
		return "", fmt.Errorf("failed to marshal payload: %w", err)
	}

	// Encode header and payload
	headerEncoded := base64.RawURLEncoding.EncodeToString(headerBytes)
	payloadEncoded := base64.RawURLEncoding.EncodeToString(payloadBytes)

	// Generate signature
	signatureData := headerEncoded + "." + payloadEncoded
	signature := a.generateSignature(signatureData)
	signatureEncoded := base64.RawURLEncoding.EncodeToString(signature)

	token := headerEncoded + "." + payloadEncoded + "." + signatureEncoded
	return token, nil
}

// isAuthenticated checks if the request context contains valid authentication
func isAuthenticated(ctx context.Context) bool {
	if auth, ok := ctx.Value(authenticatedKey).(bool); ok && auth {
		return true
	}
	return false
}

// getAuthError returns any authentication error from the context
func getAuthError(ctx context.Context) string {
	if err, ok := ctx.Value(authErrorKey).(string); ok {
		return err
	}
	return ""
}

// getUserInfo extracts user information from the authenticated context
func getUserInfo(ctx context.Context) (userID, username, role string) {
	if userID, ok := ctx.Value(userIDKey).(string); ok {
		if username, ok := ctx.Value(usernameKey).(string); ok {
			if role, ok := ctx.Value(userRoleKey).(string); ok {
				return userID, username, role
			}
		}
	}
	return "", "", ""
}

// RequireAuth is a utility function to check authentication and return error if not authenticated
func RequireAuth(ctx context.Context) error {
	if !isAuthenticated(ctx) {
		if authError := getAuthError(ctx); authError != "" {
			switch authError {
			case "missing_token":
				return fmt.Errorf("authentication required: missing or invalid authorization header")
			case "invalid_token":
				return fmt.Errorf("authentication required: invalid token")
			case "expired_token":
				return fmt.Errorf("authentication required: token expired")
			default:
				return fmt.Errorf("authentication required: %s", authError)
			}
		}
		return fmt.Errorf("authentication required")
	}
	return nil
}

// CreateTokenCommand creates a command-line utility to generate tokens
func CreateTokenCommand(secretKey, userID, username, role string, expirationHours int) {
	if secretKey == "" {
		fmt.Fprintln(os.Stderr, "Error: SECRET_KEY environment variable is required")
		return
	}

	logger := NewLogger(LevelInfo)
	auth := NewAuthMiddleware(secretKey, true, logger)

	token, err := auth.GenerateToken(userID, username, role, expirationHours)
	if err != nil {
		fmt.Fprintf(os.Stderr, "Error generating token: %v\n", err)
		return
	}

	fmt.Fprintf(os.Stderr, "Generated JWT token:\n%s\n\n", token)
	fmt.Fprintf(os.Stderr, "Token details:\n")
	fmt.Fprintf(os.Stderr, "  User ID: %s\n", userID)
	fmt.Fprintf(os.Stderr, "  Username: %s\n", username)
	fmt.Fprintf(os.Stderr, "  Role: %s\n", role)
	fmt.Fprintf(os.Stderr, "  Expires: %s\n", time.Now().Add(time.Duration(expirationHours)*time.Hour).Format(time.RFC3339))
	fmt.Fprintf(os.Stderr, "\nTo use this token, include it in HTTP requests:\n")
	fmt.Fprintf(os.Stderr, "  Authorization: Bearer %s\n", token)
}
</file>

<file path="config_test.go">
package main

import (
	"os"
	"testing"
	"time"
)

func TestNewConfig(t *testing.T) {
	// Save original environment and restore it after the test
	originalAPIKey := os.Getenv("GEMINI_API_KEY")
	originalModel := os.Getenv("GEMINI_MODEL")
	originalTimeout := os.Getenv("GEMINI_TIMEOUT")
	defer func() {
		os.Setenv("GEMINI_API_KEY", originalAPIKey)
		os.Setenv("GEMINI_MODEL", originalModel)
		os.Setenv("GEMINI_TIMEOUT", originalTimeout)
	}()

	t.Run("missing API key returns error", func(t *testing.T) {
		os.Unsetenv("GEMINI_API_KEY")

		config, err := NewConfig()

		if err == nil {
			t.Error("Expected error when API key is missing, got nil")
		}
		if config != nil {
			t.Errorf("Expected nil config when API key is missing, got %+v", config)
		}
	})

	t.Run("valid API key creates config", func(t *testing.T) {
		os.Setenv("GEMINI_API_KEY", "test-api-key")
		os.Setenv("GEMINI_MODEL", "gemini-2.5-pro") // Use a valid model from models.go

		config, err := NewConfig()

		if err != nil {
			t.Errorf("Unexpected error: %v", err)
		}
		if config == nil {
			t.Fatal("Expected config to be created, got nil")
		}
		if config.GeminiAPIKey != "test-api-key" {
			t.Errorf("Expected API key 'test-api-key', got '%s'", config.GeminiAPIKey)
		}
		if config.GeminiModel != "gemini-2.5-pro" {
			t.Errorf("Expected model 'gemini-2.5-pro', got '%s'", config.GeminiModel)
		}
		if config.HTTPTimeout != 90*time.Second {
			t.Errorf("Expected timeout of 90s, got %v", config.HTTPTimeout)
		}
	})

	t.Run("missing model uses default", func(t *testing.T) {
		os.Setenv("GEMINI_API_KEY", "test-api-key")
		os.Unsetenv("GEMINI_MODEL")

		config, err := NewConfig()

		if err != nil {
			t.Errorf("Unexpected error: %v", err)
		}
		if config == nil {
			t.Fatal("Expected config to be created, got nil")
		}
		if config.GeminiModel != "gemini-2.5-pro" {
			t.Errorf("Expected default model 'gemini-2.5-pro', got '%s'", config.GeminiModel)
		}
	})

	t.Run("custom timeout", func(t *testing.T) {
		os.Setenv("GEMINI_API_KEY", "test-api-key")
		os.Setenv("GEMINI_TIMEOUT", "180s")

		config, err := NewConfig()

		if err != nil {
			t.Errorf("Unexpected error: %v", err)
		}
		if config == nil {
			t.Fatal("Expected config to be created, got nil")
		}
		if config.HTTPTimeout != 180*time.Second {
			t.Errorf("Expected timeout of 120s, got %v", config.HTTPTimeout)
		}
	})

	t.Run("custom retry settings", func(t *testing.T) {
		os.Setenv("GEMINI_API_KEY", "test-api-key")
		os.Setenv("GEMINI_MAX_RETRIES", "3")
		os.Setenv("GEMINI_INITIAL_BACKOFF", "2s")
		os.Setenv("GEMINI_MAX_BACKOFF", "15s")

		config, err := NewConfig()

		if err != nil {
			t.Errorf("Unexpected error: %v", err)
		}
		if config == nil {
			t.Fatal("Expected config to be created, got nil")
		}
		if config.MaxRetries != 3 {
			t.Errorf("Expected max retries of 3, got %d", config.MaxRetries)
		}
		if config.InitialBackoff != 2*time.Second {
			t.Errorf("Expected initial backoff of 2s, got %v", config.InitialBackoff)
		}
		if config.MaxBackoff != 15*time.Second {
			t.Errorf("Expected max backoff of 15s, got %v", config.MaxBackoff)
		}
	})
}
func TestConfigDefaults(t *testing.T) {
	os.Clearenv()
	os.Setenv("GEMINI_API_KEY", "key")
	cfg, err := NewConfig()
	if err != nil {
		t.Fatalf("unexpected error: %v", err)
	}
	if cfg.GeminiModel != defaultGeminiModel {
		t.Errorf("default model: want %q, got %q", defaultGeminiModel, cfg.GeminiModel)
	}
	if cfg.GeminiSearchModel != defaultGeminiSearchModel {
		t.Errorf("default search model: want %q, got %q", defaultGeminiSearchModel, cfg.GeminiSearchModel)
	}
	if cfg.GeminiTemperature != defaultGeminiTemperature {
		t.Errorf("default temperature: want %v, got %v", defaultGeminiTemperature, cfg.GeminiTemperature)
	}
	if cfg.HTTPTimeout != 90*time.Second {
		t.Errorf("default timeout: want 90s, got %v", cfg.HTTPTimeout)
	}
	if cfg.MaxRetries != 2 {
		t.Errorf("default max retries: want 2, got %d", cfg.MaxRetries)
	}
	if cfg.InitialBackoff != 1*time.Second {
		t.Errorf("default initial backoff: want 1s, got %v", cfg.InitialBackoff)
	}
	if cfg.MaxBackoff != 10*time.Second {
		t.Errorf("default max backoff: want 10s, got %v", cfg.MaxBackoff)
	}
	if cfg.MaxFileSize != defaultMaxFileSize {
		t.Errorf("default max file size: want %d, got %d", defaultMaxFileSize, cfg.MaxFileSize)
	}
	if !cfg.EnableCaching {
		t.Errorf("default caching: want true, got false")
	}
	if cfg.DefaultCacheTTL != defaultDefaultCacheTTL {
		t.Errorf("default cache TTL: want %v, got %v", defaultDefaultCacheTTL, cfg.DefaultCacheTTL)
	}
	if !cfg.EnableThinking {
		t.Errorf("default thinking: want true, got false")
	}
	if cfg.ThinkingBudgetLevel != defaultThinkingBudgetLevel {
		t.Errorf("default thinking level: want %q, got %q", defaultThinkingBudgetLevel, cfg.ThinkingBudgetLevel)
	}
	expBudget := getThinkingBudgetFromLevel(defaultThinkingBudgetLevel)
	if cfg.ThinkingBudget != expBudget {
		t.Errorf("default thinking budget: want %d, got %d", expBudget, cfg.ThinkingBudget)
	}
	if cfg.EnableHTTP {
		t.Errorf("default HTTP: want false, got true")
	}
	if cfg.HTTPAddress != defaultHTTPAddress {
		t.Errorf("default HTTP address: want %q, got %q", defaultHTTPAddress, cfg.HTTPAddress)
	}
	if cfg.HTTPPath != defaultHTTPPath {
		t.Errorf("default HTTP path: want %q, got %q", defaultHTTPPath, cfg.HTTPPath)
	}
	if cfg.HTTPStateless {
		t.Errorf("default HTTP stateless: want false, got true")
	}
	if cfg.HTTPHeartbeat != defaultHTTPHeartbeat {
		t.Errorf("default HTTP heartbeat: want %v, got %v", defaultHTTPHeartbeat, cfg.HTTPHeartbeat)
	}
	if !cfg.HTTPCORSEnabled {
		t.Errorf("default CORS: want true, got false")
	}
	if len(cfg.HTTPCORSOrigins) != 1 || cfg.HTTPCORSOrigins[0] != "*" {
		t.Errorf("default CORS origins: want [\"*\"], got %v", cfg.HTTPCORSOrigins)
	}
}

func TestInvalidTemperature(t *testing.T) {
	os.Clearenv()
	os.Setenv("GEMINI_API_KEY", "key")
	os.Setenv("GEMINI_TEMPERATURE", "1.5")
	_, err := NewConfig()
	if err == nil {
		t.Fatal("expected error for GEMINI_TEMPERATURE > 1.0, got nil")
	}
}

func TestValidTemperature(t *testing.T) {
	os.Clearenv()
	os.Setenv("GEMINI_API_KEY", "key")
	os.Setenv("GEMINI_TEMPERATURE", "0.8")
	cfg, err := NewConfig()
	if err != nil {
		t.Fatalf("unexpected error: %v", err)
	}
	if cfg.GeminiTemperature != 0.8 {
		t.Errorf("override temperature: want 0.8, got %v", cfg.GeminiTemperature)
	}
}

func TestFileSettings(t *testing.T) {
	os.Clearenv()
	os.Setenv("GEMINI_API_KEY", "key")
	os.Setenv("GEMINI_MAX_FILE_SIZE", "2097152") // 2 MB
	os.Setenv("GEMINI_ALLOWED_FILE_TYPES", "text/foo,application/bar")
	cfg, err := NewConfig()
	if err != nil {
		t.Fatalf("unexpected error: %v", err)
	}
	if cfg.MaxFileSize != 2097152 {
		t.Errorf("max file size: want 2097152, got %d", cfg.MaxFileSize)
	}
	wantTypes := []string{"text/foo", "application/bar"}
	if len(cfg.AllowedFileTypes) != 2 ||
		cfg.AllowedFileTypes[0] != wantTypes[0] ||
		cfg.AllowedFileTypes[1] != wantTypes[1] {
		t.Errorf("allowed types: want %v, got %v", wantTypes, cfg.AllowedFileTypes)
	}
}

func TestCacheSettings(t *testing.T) {
	os.Clearenv()
	os.Setenv("GEMINI_API_KEY", "key")
	os.Setenv("GEMINI_ENABLE_CACHING", "false")
	os.Setenv("GEMINI_DEFAULT_CACHE_TTL", "30m")
	cfg, err := NewConfig()
	if err != nil {
		t.Fatalf("unexpected error: %v", err)
	}
	if cfg.EnableCaching {
		t.Errorf("enable caching: want false, got true")
	}
	if cfg.DefaultCacheTTL != 30*time.Minute {
		t.Errorf("cache TTL: want 30m, got %v", cfg.DefaultCacheTTL)
	}
}

func TestThinkingSettings(t *testing.T) {
	// override level only
	os.Clearenv()
	os.Setenv("GEMINI_API_KEY", "key")
	os.Setenv("GEMINI_THINKING_BUDGET_LEVEL", "high")
	cfg, err := NewConfig()
	if err != nil {
		t.Fatalf("unexpected error: %v", err)
	}
	if cfg.ThinkingBudgetLevel != "high" {
		t.Errorf("thinking level: want high, got %s", cfg.ThinkingBudgetLevel)
	}
	if cfg.ThinkingBudget != getThinkingBudgetFromLevel("high") {
		t.Errorf("thinking budget: want %d, got %d",
			getThinkingBudgetFromLevel("high"), cfg.ThinkingBudget)
	}
	// explicit budget override
	os.Clearenv()
	os.Setenv("GEMINI_API_KEY", "key")
	os.Setenv("GEMINI_THINKING_BUDGET_LEVEL", "medium")
	os.Setenv("GEMINI_THINKING_BUDGET", "1000")
	cfg2, err := NewConfig()
	if err != nil {
		t.Fatalf("unexpected error: %v", err)
	}
	if cfg2.ThinkingBudget != 1000 {
		t.Errorf("explicit thinking budget: want 1000, got %d", cfg2.ThinkingBudget)
	}
}

func TestHTTPSettings(t *testing.T) {
	os.Clearenv()
	os.Setenv("GEMINI_API_KEY", "key")
	os.Setenv("GEMINI_ENABLE_HTTP", "true")
	os.Setenv("GEMINI_HTTP_ADDRESS", ":9090")
	os.Setenv("GEMINI_HTTP_PATH", "/test")
	os.Setenv("GEMINI_HTTP_STATELESS", "true")
	os.Setenv("GEMINI_HTTP_HEARTBEAT", "5s")
	os.Setenv("GEMINI_HTTP_CORS_ENABLED", "false")
	os.Setenv("GEMINI_HTTP_CORS_ORIGINS", "https://a,https://b")
	cfg, err := NewConfig()
	if err != nil {
		t.Fatalf("unexpected error: %v", err)
	}
	if !cfg.EnableHTTP {
		t.Error("enable HTTP: want true, got false")
	}
	if cfg.HTTPAddress != ":9090" {
		t.Errorf("HTTP address: want :9090, got %s", cfg.HTTPAddress)
	}
	if cfg.HTTPPath != "/test" {
		t.Errorf("HTTP path: want /test, got %s", cfg.HTTPPath)
	}
	if !cfg.HTTPStateless {
		t.Error("HTTP stateless: want true, got false")
	}
	if cfg.HTTPHeartbeat != 5*time.Second {
		t.Errorf("HTTP heartbeat: want 5s, got %v", cfg.HTTPHeartbeat)
	}
	if cfg.HTTPCORSEnabled {
		t.Error("CORS enabled: want false, got true")
	}
	if len(cfg.HTTPCORSOrigins) != 2 ||
		cfg.HTTPCORSOrigins[0] != "https://a" ||
		cfg.HTTPCORSOrigins[1] != "https://b" {
		t.Errorf("CORS origins: want [https://a https://b], got %v", cfg.HTTPCORSOrigins)
	}
}
</file>

<file path="model_functions.go">
package main

import (
	"fmt"
	"strings"
	"sync"
)

// modelStore handles storing and retrieving models
var modelStore struct {
	sync.RWMutex
	models []GeminiModelInfo
}

// GetAvailableGeminiModels returns a list of available Gemini models
func GetAvailableGeminiModels() []GeminiModelInfo {
	// Get models with read lock
	modelStore.RLock()
	defer modelStore.RUnlock()

	// Return cached model list if available
	if len(modelStore.models) > 0 {
		return modelStore.models
	}

	// Return fallback models if nothing has been fetched yet
	return fallbackGeminiModels()
}

// GetModelByID returns model info for either a family ID or a version ID
func GetModelByID(modelID string) *GeminiModelInfo {
	models := GetAvailableGeminiModels()

	// Check if it's a family ID
	for _, model := range models {
		if model.FamilyID == modelID {
			return &model
		}

		// Check if it's a version ID
		for _, version := range model.Versions {
			if version.ID == modelID {
				return &model // Return the family for this version
			}
		}
	}
	return nil
}

// GetModelVersion returns the specific version info for a model ID
func GetModelVersion(modelID string) *ModelVersion {
	for _, model := range GetAvailableGeminiModels() {
		for i, version := range model.Versions {
			if version.ID == modelID {
				return &model.Versions[i]
			}
		}
	}
	return nil
}

// ResolveModelID converts a model family ID or version ID to an actual API-usable version ID
// If the provided ID is already a version ID, it returns it unchanged
// If it's a family ID, it returns the ID of the preferred or first version
func ResolveModelID(modelID string) string {
	// First check if this is already a specific version ID
	if GetModelVersion(modelID) != nil {
		return modelID // Already a valid version ID
	}

	// It might be a family ID, try to find the best version
	model := GetModelByID(modelID)
	if model != nil {
		// Find preferred version first
		for _, version := range model.Versions {
			if version.IsPreferred {
				return version.ID
			}
		}

		// Otherwise return the first version
		if len(model.Versions) > 0 {
			return model.Versions[0].ID
		}
	}

	// If we get here, it's an unknown ID, return it unchanged
	return modelID
}

// ValidateModelID checks if a model ID is in the list of available models
// Returns nil if valid, error otherwise
func ValidateModelID(modelID string) error {
	// First check if it's a known version ID or family ID
	if GetModelVersion(modelID) != nil || GetModelByID(modelID) != nil {
		return nil
	}

	// Special handling for preview models or other special cases
	// Preview models often have date suffixes like "preview-04-17"
	if strings.Contains(modelID, "preview") ||
		strings.Contains(modelID, "exp") ||
		strings.HasSuffix(modelID, "-dev") {
		// Allow preview/experimental models even if not in our list
		return nil
	}

	// Model is neither in our list nor a recognized preview format
	// Return a warning, but don't block the model from being used
	var sb strings.Builder
	sb.WriteString(fmt.Sprintf("Unknown model ID: %s. Known models are:", modelID))
	for _, model := range GetAvailableGeminiModels() {
		sb.WriteString(fmt.Sprintf("\n- %s: %s", model.FamilyID, model.Name))
		for _, version := range model.Versions {
			sb.WriteString(fmt.Sprintf("\n  - %s: %s", version.ID, version.Name))
		}
	}
	sb.WriteString("\n\nHowever, we will attempt to use this model anyway. It may be a new or preview model.")

	return fmt.Errorf("%s", sb.String())
}
</file>

<file path="files.go">
package main

import (
	"bytes"
	"context"
	"errors"
	"fmt"
	"strings"
	"time"

	"google.golang.org/genai"
)

// FileUploadRequest struct definition moved to structs.go

// FileInfo struct definition moved to structs.go
// FileStore struct definition moved to structs.go

// NewFileStore creates a new file store
func NewFileStore(client *genai.Client, config *Config) *FileStore {
	return &FileStore{
		client:   client,
		config:   config,
		fileInfo: make(map[string]*FileInfo),
	}
}

// UploadFile uploads a file to the Gemini API
func (fs *FileStore) UploadFile(ctx context.Context, req *FileUploadRequest) (*FileInfo, error) {
	// Get logger from context
	logger := getLoggerFromContext(ctx)

	// Input validation
	if req.FileName == "" {
		return nil, errors.New("filename is required")
	}
	if req.MimeType == "" {
		return nil, errors.New("mime type is required")
	}
	if len(req.Content) == 0 {
		return nil, errors.New("content is required")
	}

	// Validate file size
	if int64(len(req.Content)) > fs.config.MaxFileSize {
		return nil, fmt.Errorf("file size exceeds maximum allowed (%d bytes)", fs.config.MaxFileSize)
	}

	// Validate mime type
	mimeTypeAllowed := false
	for _, allowedType := range fs.config.AllowedFileTypes {
		if req.MimeType == allowedType {
			mimeTypeAllowed = true
			break
		}
	}
	if !mimeTypeAllowed {
		return nil, fmt.Errorf("mime type %s is not allowed", req.MimeType)
	}

	// Check if client is properly initialized
	if fs.client == nil || fs.client.Files == nil {
		logger.Error("Gemini client or Files service not properly initialized")
		return nil, errors.New("internal error: Gemini client not properly initialized")
	}

	// Create options with display name if provided
	opts := &genai.UploadFileConfig{
		MIMEType: req.MimeType,
	}
	if req.DisplayName != "" {
		opts.DisplayName = req.DisplayName
	} else {
		// Use filename as display name if not provided
		opts.DisplayName = req.FileName
	}

	// Upload file to Gemini API
	logger.Info("Uploading file %s with MIME type %s", req.FileName, req.MimeType)
	file, err := fs.client.Files.Upload(ctx, bytes.NewReader(req.Content), opts)
	if err != nil {
		logger.Error("Failed to upload file: %v", err)
		return nil, fmt.Errorf("failed to upload file: %w", err)
	}

	// Extract ID from name (format: "files/abc123")
	id := file.Name
	if strings.HasPrefix(file.Name, "files/") {
		id = strings.TrimPrefix(file.Name, "files/")
	}

	// Create file info
	fileInfo := &FileInfo{
		ID:          id,
		Name:        file.Name,
		URI:         file.URI,
		DisplayName: file.DisplayName,
		MimeType:    file.MIMEType,
		Size:        0, // SizeBytes is now a pointer in the new API
		UploadedAt:  file.CreateTime,
	}

	// Set size if available
	if file.SizeBytes != nil {
		fileInfo.Size = *file.SizeBytes
	} else {
		// If not available, use the content length
		fileInfo.Size = int64(len(req.Content))
	}

	// Set expiration if provided
	if !file.ExpirationTime.IsZero() {
		fileInfo.ExpiresAt = file.ExpirationTime
	} else {
		// Default expiration if not provided by API
		fileInfo.ExpiresAt = time.Now().Add(24 * time.Hour)
	}

	// Store file info
	// Validate URI before storing
	if fileInfo.URI == "" {
		logger.Error("Invalid URI for uploaded file: empty URI")
		return nil, errors.New("invalid URI for uploaded file")
	}

	logger.Debug("Storing file info with URI: %s", fileInfo.URI)
	fs.mu.Lock()
	fs.fileInfo[id] = fileInfo
	fs.mu.Unlock()

	logger.Info("File uploaded successfully with ID: %s", id)
	return fileInfo, nil
}

// GetFile gets file information by ID
func (fs *FileStore) GetFile(ctx context.Context, id string) (*FileInfo, error) {
	logger := getLoggerFromContext(ctx)

	// Validate client first
	if fs.client == nil {
		return nil, errors.New("file store client is nil")
	}

	// Check cache first
	fs.mu.RLock()
	info, ok := fs.fileInfo[id]
	fs.mu.RUnlock()

	if ok {
		logger.Debug("File info for %s found in cache", id)
		return info, nil
	}

	// If not in cache, try to get from API
	name := id
	if !strings.HasPrefix(id, "files/") {
		name = "files/" + id
	}

	// Check if client is properly initialized
	if fs.client == nil || fs.client.Files == nil {
		logger.Error("Gemini client or Files service not properly initialized")
		return nil, errors.New("internal error: Gemini client not properly initialized")
	}

	logger.Info("Fetching file info for %s from API", name)
	file, err := fs.client.Files.Get(ctx, name, nil)
	if err != nil {
		logger.Error("Failed to get file from API: %v", err)
		return nil, fmt.Errorf("failed to get file: %w", err)
	}

	// Extract ID from name
	fileID := file.Name
	if strings.HasPrefix(file.Name, "files/") {
		fileID = strings.TrimPrefix(file.Name, "files/")
	}

	// Create file info
	fileInfo := &FileInfo{
		ID:          fileID,
		Name:        file.Name,
		URI:         file.URI,
		DisplayName: file.DisplayName,
		MimeType:    file.MIMEType,
		Size:        0, // SizeBytes is now a pointer in the new API
		UploadedAt:  file.CreateTime,
	}

	// Set size if available
	if file.SizeBytes != nil {
		fileInfo.Size = *file.SizeBytes
	}

	// Set expiration if provided
	if !file.ExpirationTime.IsZero() {
		fileInfo.ExpiresAt = file.ExpirationTime
	}

	// Store in cache
	fs.mu.Lock()
	fs.fileInfo[fileID] = fileInfo
	fs.mu.Unlock()

	logger.Debug("Added file info for %s to cache", fileID)
	return fileInfo, nil
}

// Helper function to format file sizes in a human-readable way
func humanReadableSize(bytes int64) string {
	const unit = 1024
	if bytes < unit {
		return fmt.Sprintf("%d B", bytes)
	}

	div, exp := int64(unit), 0
	for n := bytes / unit; n >= unit; n /= unit {
		div *= unit
		exp++
	}

	return fmt.Sprintf("%.1f %cB", float64(bytes)/float64(div), "KMGTPE"[exp])
}

// Helper function to get MIME type from file path
// Moved to gemini_utils.go
</file>

<file path="prompts.go">
package main

// Prompts defines all the available prompts for the server.
var Prompts = []*PromptDefinition{
	NewPromptDefinition(
		"code_review",
		"Review code for best practices, potential issues, and improvements",
		`You are an expert code reviewer with years of experience in software engineering. Your task is to conduct a thorough analysis of the provided code.

Focus on the following areas:
- **Code Quality & Best Practices:** Adherence to language-specific idioms, code formatting, and established best practices.
- **Potential Bugs:** Logical errors, race conditions, null pointer issues, and other potential bugs.
- **Security Vulnerabilities:** Identify any potential security risks, such as injection vulnerabilities, insecure data handling, or authentication/authorization flaws. Follow OWASP Top 10 guidelines.
- **Performance Concerns:** Look for inefficient algorithms, memory leaks, or other performance bottlenecks.
- **Maintainability & Readability:** Assess the code's clarity, modularity, and ease of maintenance.

Provide specific, actionable feedback. For each issue, include the file path (if available), the relevant line number(s), and a clear explanation of the problem and your suggested improvement.`,
	),
	NewPromptDefinition(
		"explain_code",
		"Explain how code works in detail, including algorithms and design patterns",
		`You are an expert software engineer and a skilled educator. Your goal is to explain the provided code in a clear, comprehensive, and easy-to-understand manner.

Structure your explanation as follows:
1.  **High-Level Overview:** Start with a summary of what the code does and its primary purpose.
2.  **Detailed Breakdown:** Go through the code section by section, explaining the logic, algorithms, and data structures used.
3.  **Key Concepts:** Highlight any important design patterns, architectural decisions, or programming concepts demonstrated in the code.
4.  **Usage:** If applicable, provide a simple example of how to use the code.

Tailor the complexity of your explanation to be suitable for an intermediate-level developer.`,
	),
	NewPromptDefinition(
		"debug_help",
		"Help debug issues by analyzing code, error messages, and context",
		`You are an expert debugger. Your mission is to analyze the provided code and the user's problem description to identify the root cause of a bug and suggest a solution.

Follow this systematic debugging process:
1.  **Analyze the Code:** Carefully review the provided code for potential logical errors, incorrect assumptions, or other issues related to the problem description.
2.  **Identify the Root Cause:** Based on your analysis, pinpoint the most likely cause of the bug.
3.  **Propose a Fix:** Provide a specific, corrected code snippet to fix the bug.
4.  **Explain the Solution:** Clearly explain why the bug occurred and why your proposed solution resolves it.`,
	),
	NewPromptDefinition(
		"refactor_suggestions",
		"Suggest improvements and refactoring opportunities for existing code",
		`You are an expert software architect specializing in code modernization and refactoring. Your task is to analyze the provided code and suggest concrete improvements.

Your suggestions should focus on:
- **Improving Code Structure:** Enhancing modularity, separation of concerns, and overall organization.
- **Applying Design Patterns:** Identifying opportunities to use appropriate design patterns to solve common problems.
- **Increasing Readability & Maintainability:** Making the code easier to understand and modify in the future.
- **Optimizing Performance:** Where applicable, suggest changes to improve efficiency without sacrificing clarity.

For each suggestion, provide a code example demonstrating the change and explain the benefits of the proposed refactoring.`,
	),
	NewPromptDefinition(
		"architecture_analysis",
		"Analyze system architecture, design patterns, and structural decisions",
		`You are a seasoned software architect. Your task is to conduct a high-level analysis of the provided codebase to understand its architecture.

Your analysis should cover:
- **Overall Design:** Describe the main architectural pattern (e.g., Monolith, Microservices, MVC, etc.).
- **Component Breakdown:** Identify the key components, their responsibilities, and how they interact.
- **Data Flow:** Explain how data flows through the system.
- **Dependencies:** List the major external dependencies and their roles.
- **Potential Issues:** Highlight any potential architectural weaknesses, bottlenecks, or areas for improvement regarding scalability, maintainability, or security.

Provide a clear and concise summary of the architecture.`,
	),
	NewPromptDefinition(
		"doc_generate",
		"Generate comprehensive documentation for code, APIs, or systems",
		`You are a professional technical writer. Your task is to generate clear, concise, and comprehensive documentation for the provided code.

The documentation should be in Markdown format and include the following sections for each major component or function:
- **Purpose:** A brief description of what the code does.
- **Parameters:** A list of all input parameters, their types, and a description of each.
- **Return Value:** A description of what the function or component returns.
- **Usage Example:** A simple code snippet demonstrating how to use the code.

Ensure the documentation is accurate and easy for other developers to understand.`,
	),
	NewPromptDefinition(
		"test_generate",
		"Generate unit tests, integration tests, or test cases for code",
		`You are a test engineering expert. Your task is to generate comprehensive unit tests for the provided code.

The generated tests should:
- Be written using the standard testing library for the given language.
- Cover happy-path scenarios, edge cases, and error conditions.
- Follow best practices for testing, including clear test descriptions, and proper assertions.
- Be easy to read and maintain.

For each function or method, provide a set of corresponding test cases.`,
	),
	NewPromptDefinition(
		"security_analysis",
		"Analyze code for security vulnerabilities and best practices",
		`You are a cybersecurity expert specializing in secure code review. Your task is to analyze the provided code for security vulnerabilities and risks.

Focus on identifying common vulnerabilities, including but not limited to:
- Injection attacks (SQL, Command, etc.)
- Cross-Site Scripting (XSS)
- Insecure Deserialization
- Broken Authentication and Access Control
- Security Misconfiguration
- Sensitive Data Exposure

For each vulnerability you identify, provide:
- A description of the vulnerability and its potential impact.
- The file path and line number where the vulnerability exists.
- A clear recommendation on how to remediate the vulnerability, including a corrected code snippet where possible.`,
	),
	NewPromptDefinition(
		"research_question",
		"Research current information and trends using Google Search integration",
		"", // Use default search system prompt from config
	),
}
</file>

<file path="gemini_server.go">
package main

import (
	"context"
	"errors"
	"fmt"

	"google.golang.org/genai"
)

// NewGeminiServer creates a new GeminiServer with the provided configuration
func NewGeminiServer(ctx context.Context, config *Config) (*GeminiServer, error) {
	if config == nil {
		return nil, errors.New("config cannot be nil")
	}

	if config.GeminiAPIKey == "" {
		return nil, errors.New("gemini API key is required")
	}

	// Initialize the Gemini client
	clientConfig := &genai.ClientConfig{
		APIKey: config.GeminiAPIKey,
	}
	client, err := genai.NewClient(ctx, clientConfig)
	if err != nil {
		return nil, fmt.Errorf("failed to create Gemini client: %w", err)
	}

	// Create the file and cache stores
	fileStore := NewFileStore(client, config)
	cacheStore := NewCacheStore(client, config, fileStore)

	return &GeminiServer{
		config:     config,
		client:     client,
		fileStore:  fileStore,
		cacheStore: cacheStore,
	}, nil
}
</file>

<file path="fallback_models.go">
package main

// fallbackGeminiModels provides the 3 actual Gemini 2.5 models as documented by Google
func fallbackGeminiModels() []GeminiModelInfo {
	return []GeminiModelInfo{
		// Gemini 2.5 Pro - Production model
		{
			FamilyID:             "gemini-2.5-pro",
			Name:                 "Gemini 2.5 Pro",
			Description:          "Our most powerful thinking model with maximum response accuracy and state-of-the-art performance",
			SupportsThinking:     true,
			ContextWindowSize:    1048576,
			PreferredForThinking: true,
			PreferredForCaching:  true,
			PreferredForSearch:   false,
			Versions:             []ModelVersion{}, // Production model uses family ID directly
		},

		// Gemini 2.5 Flash - Production model
		{
			FamilyID:             "gemini-2.5-flash",
			Name:                 "Gemini 2.5 Flash",
			Description:          "Best model in terms of price-performance, offering well-rounded capabilities",
			SupportsThinking:     true,
			ContextWindowSize:    32768,
			PreferredForThinking: false,
			PreferredForCaching:  true,
			PreferredForSearch:   false,
			Versions:             []ModelVersion{}, // Production model uses family ID directly
		},

		// Gemini 2.5 Flash Lite - GA model
		{
			FamilyID:             "gemini-2.5-flash-lite",
			Name:                 "Gemini 2.5 Flash Lite",
			Description:          "Optimized for cost efficiency and low latency",
			SupportsThinking:     true,
			ContextWindowSize:    32768,
			PreferredForThinking: false,
			PreferredForCaching:  false,
			PreferredForSearch:   true,
			Versions:             []ModelVersion{}, // Production model uses family ID directly
		},
	}
}
</file>

<file path="prompts_test.go">
package main

import (
	"context"
	"strings"
	"testing"

	"github.com/mark3labs/mcp-go/mcp"
)

// TestPromptHandlers is a table-driven test that covers all prompt handlers.
func TestPromptHandlers(t *testing.T) {
	// Create a test config and server instance.
	config := &Config{
		GeminiAPIKey: "test-key",
		GeminiModel:  "gemini-2.5-pro",
	}
	ctx := context.Background()
	ctx = context.WithValue(ctx, loggerKey, NewLogger(LevelInfo))
	geminiSvc, err := NewGeminiServer(ctx, config)
	if err != nil {
		t.Fatalf("Skipping tests: could not create GeminiServer instance: %v", err)
	}

	problemStatement := "Please check my code for potential issues."

	for _, p := range Prompts {
		t.Run(p.Name, func(t *testing.T) {
			req := mcp.GetPromptRequest{
				Params: mcp.GetPromptParams{
					Name: p.Name,
					Arguments: map[string]string{
						"problem_statement": problemStatement,
					},
				},
			}

			handler := geminiSvc.promptHandler(p)
			result, err := handler(ctx, req)
			if err != nil {
				t.Fatalf("Handler returned an unexpected error: %v", err)
			}

			if result == nil {
				t.Fatal("Handler returned a nil result")
			}

			textContent, ok := result.Messages[0].Content.(mcp.TextContent)
			if !ok {
				t.Fatal("Expected message content to be TextContent")
			}
			instructions := textContent.Text

			if !strings.Contains(instructions, problemStatement) {
				t.Errorf("Expected instructions to contain the problem statement, but it was missing")
			}

			// Verify that the instructions contain the system prompt.
			if !strings.Contains(instructions, p.SystemPrompt.GetSystemPrompt()) {
				t.Errorf("Expected instructions to contain the system prompt, but it was missing")
			}
		})
	}
}
</file>

<file path="CLAUDE.md">
# Project Context: GeminiMCP Server

## Overview
This project is a Go-based MCP (Model Control Protocol) server that acts as a bridge to Google's Gemini API. It's designed as a single, self-contained binary for easy deployment and use with MCP-compatible clients. The server exclusively supports the Gemini 2.5 family of models.

## Architecture
- **Language**: Go (Golang)
- **Main Entrypoint**: `main.go`
- **Configuration**: `config.go` (environment variables with CLI overrides)
- **Core Logic**:
    - `gemini_server.go`: Gemini service implementation.
    - `direct_handlers.go`: Handlers for the MCP tools.
    - `prompt_handlers.go`: Handlers for MCP prompts.
    - `tools.go`: Definitions of the MCP tools.
- **Transport**: Supports `stdio` and `http` (with JWT authentication).
- **Dependencies**:
    - `github.com/mark3labs/mcp-go/mcp`: MCP protocol implementation.
    - `github.com/mark3labs/mcp-go/server`: MCP server implementation.
    - `google.golang.org/genai`: Google Gemini API client.
    - `github.com/joho/godotenv`: for loading `.env` files.

## Development Guidelines
- **Build**: `go build -o ./bin/mcp-gemini .`
- **Testing**: `./run_test.sh`
- **Formatting**: `./run_format.sh`
- **Linting**: `./run_lint.sh`
- **Error Handling**: The server has a "degraded mode" to handle initialization errors gracefully.
- **Logging**: A custom logger is used throughout the application.

## AI Assistant Guidelines
- When adding a new tool, define it in `tools.go`, implement the handler in `direct_handlers.go`, and register it in `setupGeminiServer()` in `main.go`.
- When adding a new prompt, define it in `prompts.go`, implement the handler in `prompt_handlers.go` using the `server.PromptHandlerFunc` type, and register it in `setupGeminiServer()`.
- When modifying configuration, update `config.go` for defaults, `NewConfig()` for parsing, `structs.go` for the `Config` struct, and `main.go` for CLI flags.
- Always use `ResolveModelID()` before making API calls to convert model family IDs to specific version IDs.
- Use the existing logging infrastructure for any new logging.
- Follow the existing code style and patterns.
</file>

<file path="handlers_common.go">
package main

import (
	"context"
	"fmt"

	"github.com/mark3labs/mcp-go/mcp"
	"google.golang.org/genai"
)

// extractArgumentString extracts a string argument from the request parameters
func extractArgumentString(req mcp.CallToolRequest, name string, defaultValue string) string {
	args := req.GetArguments()
	if val, ok := args[name].(string); ok && val != "" {
		return val
	}
	return defaultValue
}

// extractArgumentBool extracts a boolean argument from the request parameters
func extractArgumentBool(req mcp.CallToolRequest, name string, defaultValue bool) bool {
	args := req.GetArguments()
	if val, ok := args[name].(bool); ok {
		return val
	}
	return defaultValue
}

// This function has been removed as it was unused after refactoring to use direct handlers with mcp-go types

// extractArgumentStringArray extracts a string array argument from the request parameters
func extractArgumentStringArray(req mcp.CallToolRequest, name string) []string {
	var result []string
	args := req.GetArguments()
	if rawArray, ok := args[name].([]interface{}); ok {
		for _, item := range rawArray {
			if str, ok := item.(string); ok {
				result = append(result, str)
			}
		}
	}
	return result
}

// createModelConfig creates a GenerateContentConfig for Gemini API based on request parameters
func createModelConfig(ctx context.Context, req mcp.CallToolRequest, config *Config, defaultModel string) (*genai.GenerateContentConfig, string, error) {
	logger := getLoggerFromContext(ctx)

	// Extract model parameter - use defaultModel if not specified
	modelName := extractArgumentString(req, "model", defaultModel)

	// Validate the model
	if err := ValidateModelID(modelName); err != nil {
		logger.Error("Invalid model requested: %v", err)
		return nil, "", fmt.Errorf("invalid model specified: %v", err)
	}

	// Resolve model ID to ensure we use a valid API-addressable version
	resolvedModelID := ResolveModelID(modelName)
	if resolvedModelID != modelName {
		logger.Info("Resolved model ID from '%s' to '%s'", modelName, resolvedModelID)
		modelName = resolvedModelID
	}

	// Extract system prompt
	systemPrompt := config.GeminiSystemPrompt
	if sp, ok := req.GetArguments()["systemPrompt"].(string); ok {
		systemPrompt = sp
	}

	// Get model information
	modelInfo := GetModelByID(modelName)
	if modelInfo == nil {
		logger.Warn("Model information not found for %s, using default parameters", modelName)
	}

	// Create the configuration
	contentConfig := &genai.GenerateContentConfig{
		SystemInstruction: genai.NewContentFromText(systemPrompt, ""),
		Temperature:       genai.Ptr(float32(config.GeminiTemperature)),
	}

	// Configure thinking if supported
	enableThinking := extractArgumentBool(req, "enable_thinking", config.EnableThinking)
	if enableThinking && modelInfo != nil && modelInfo.SupportsThinking {
		thinkingConfig := &genai.ThinkingConfig{
			IncludeThoughts: true,
		}

		// Determine thinking budget
		thinkingBudget := 0

		// Check for level first
		args := req.GetArguments()
		if levelStr, ok := args["thinking_budget_level"].(string); ok && levelStr != "" {
			thinkingBudget = getThinkingBudgetFromLevel(levelStr)
			logger.Info("Setting thinking budget to %d tokens from level: %s", thinkingBudget, levelStr)
		} else if budgetRaw, ok := args["thinking_budget"].(float64); ok && budgetRaw >= 0 {
			// If explicit budget was provided, use that instead of level
			thinkingBudget = int(budgetRaw)
			logger.Info("Setting thinking budget to %d tokens from explicit value", thinkingBudget)
		} else if config.ThinkingBudget > 0 {
			// Fall back to config value if neither level nor explicit budget provided
			thinkingBudget = config.ThinkingBudget
			logger.Info("Using default thinking budget of %d tokens", thinkingBudget)
		}

		// Set budget if greater than 0
		if thinkingBudget > 0 {
			budget := int32(thinkingBudget)
			thinkingConfig.ThinkingBudget = &budget
		}

		contentConfig.ThinkingConfig = thinkingConfig
		logger.Info("Thinking mode enabled with budget %d for model %s", thinkingBudget, modelName)
	} else if enableThinking && (modelInfo == nil || !modelInfo.SupportsThinking) {
		logger.Warn("Thinking mode was requested but model doesn't support it")
	}

	// Configure max tokens with default ratio of context window
	configureMaxTokensOutput(ctx, contentConfig, req, modelInfo, 0.75)

	return contentConfig, modelName, nil
}

// configureMaxTokensOutput configures the maximum output tokens for the request
func configureMaxTokensOutput(ctx context.Context, config *genai.GenerateContentConfig, req mcp.CallToolRequest, modelInfo *GeminiModelInfo, defaultRatio float64) {
	logger := getLoggerFromContext(ctx)

	// Check if max_tokens parameter was provided
	args := req.GetArguments()
	if maxTokensRaw, ok := args["max_tokens"].(float64); ok && maxTokensRaw > 0 {
		maxTokens := int(maxTokensRaw)

		// Warn if tokens exceed the model's context window
		if modelInfo != nil && maxTokens > modelInfo.ContextWindowSize {
			logger.Warn("Requested max_tokens (%d) exceeds model's context window size (%d)",
				maxTokens, modelInfo.ContextWindowSize)
		}

		// Set the maximum output token limit
		config.MaxOutputTokens = int32(maxTokens)
		logger.Info("Setting max output tokens to %d", maxTokens)
	} else if modelInfo != nil {
		// Set a safe default if not specified using the provided ratio
		safeTokenLimit := int32(float64(modelInfo.ContextWindowSize) * defaultRatio)
		config.MaxOutputTokens = safeTokenLimit
		logger.Debug("Using default max output tokens: %d (%.0f%% of context window)",
			safeTokenLimit, defaultRatio*100)
	}
}

// createErrorResult creates a standardized error result for mcp.CallToolResult
func createErrorResult(message string) *mcp.CallToolResult {
	return mcp.NewToolResultError(message)
}

// convertGenaiResponseToMCPResult converts a Gemini API response to an MCP result
func convertGenaiResponseToMCPResult(resp *genai.GenerateContentResponse) *mcp.CallToolResult {
	// Check for empty response
	if resp == nil || len(resp.Candidates) == 0 || resp.Candidates[0].Content == nil {
		return mcp.NewToolResultError("Gemini API returned an empty response")
	}

	// Get the text from the response
	text := resp.Text()
	if text == "" {
		text = "The Gemini model returned an empty response. This might indicate that the model couldn't generate an appropriate response for your query. Please try rephrasing your question or providing more context."
	}

	// The 'thinking' field is not directly available in the Go client library.
	// The response will be plain text. If thinking was enabled, the model's
	// reasoning might be part of the main text response, but it cannot be
	// separated into a distinct field.

	// Return simple text response
	return &mcp.CallToolResult{
		Content: []mcp.Content{
			mcp.NewTextContent(text),
		},
	}
}
</file>

<file path="fetch_models.go">
package main

import (
	"context"
)

// FetchGeminiModels simply uses the predefined fallback models since we only support
// the 3 specific Gemini 2.5 models: Pro, Flash, and Flash Lite
func FetchGeminiModels(ctx context.Context, apiKey string) error {
	// Get logger from context if available
	var logger Logger
	loggerValue := ctx.Value(loggerKey)
	if loggerValue != nil {
		if l, ok := loggerValue.(Logger); ok {
			logger = l
		} else {
			logger = NewLogger(LevelInfo)
		}
	} else {
		logger = NewLogger(LevelInfo)
	}

	logger.Info("Setting up Gemini 2.5 model families...")

	// Use the 3 predefined Gemini 2.5 models
	models := fallbackGeminiModels()

	// Update model store with write lock
	modelStore.Lock()
	modelStore.models = models
	modelStore.Unlock()

	logger.Info("Successfully configured %d Gemini 2.5 model families", len(models))

	// Log the configured models for easier debugging
	for i, model := range models {
		logger.Debug("Model family %d: %s (%s)", i+1, model.FamilyID, model.Name)
		for j, version := range model.Versions {
			logger.Debug("  Version %d.%d: %s (%s)", i+1, j+1, version.ID, version.Name)
		}
	}

	return nil
}
</file>

<file path="gemini_utils.go">
package main

import (
	"context"
	"path/filepath"
	"strings"
)

// getLoggerFromContext safely extracts a logger from the context or creates a new one
func getLoggerFromContext(ctx context.Context) Logger {
	loggerValue := ctx.Value(loggerKey)
	if loggerValue != nil {
		if l, ok := loggerValue.(Logger); ok {
			return l
		}
	}
	// Create a new logger if one isn't in the context or type assertion fails
	return NewLogger(LevelInfo)
}

// This function has been removed after refactoring to use direct MCP types

// This function has been removed after refactoring to use formatMCPResponse and direct MCP types

// Helper function to get MIME type from file path
func getMimeTypeFromPath(path string) string {
	ext := strings.ToLower(filepath.Ext(path))

	switch ext {
	case ".txt":
		return "text/plain"
	case ".html", ".htm":
		return "text/html"
	case ".css":
		return "text/css"
	case ".js":
		return "application/javascript"
	case ".json":
		return "application/json"
	case ".xml":
		return "application/xml"
	case ".pdf":
		return "application/pdf"
	case ".png":
		return "image/png"
	case ".jpg", ".jpeg":
		return "image/jpeg"
	case ".gif":
		return "image/gif"
	case ".svg":
		return "image/svg+xml"
	case ".mp3":
		return "audio/mpeg"
	case ".mp4":
		return "video/mp4"
	case ".wav":
		return "audio/wav"
	case ".doc", ".docx":
		return "application/msword"
	case ".xls", ".xlsx":
		return "application/vnd.ms-excel"
	case ".ppt", ".pptx":
		return "application/vnd.ms-powerpoint"
	case ".zip":
		return "application/zip"
	case ".csv":
		return "text/csv"
	case ".go":
		return "text/plain" // Changed from "text/x-go" to "text/plain"
	case ".py":
		return "text/plain" // Changed from "text/x-python" to "text/plain"
	case ".java":
		return "text/plain" // Changed from "text/x-java" to "text/plain"
	case ".c", ".cpp", ".h", ".hpp":
		return "text/plain" // Changed from "text/x-c" to "text/plain"
	case ".rb":
		return "text/plain"
	case ".php":
		return "text/plain"
	case ".md":
		return "text/markdown"
	default:
		return "application/octet-stream"
	}
}

// This function has been removed as it was unused after refactoring to use convertGenaiResponseToMCPResult
</file>

<file path="prompt_handlers.go">
package main

import (
	"context"
	"fmt"

	"github.com/mark3labs/mcp-go/mcp"
	"github.com/mark3labs/mcp-go/server"
)

// createTaskInstructions generates the instructional text for the MCP client
func createTaskInstructions(problemStatement, systemPrompt string) string {
	return fmt.Sprintf("You MUST use the `gemini_ask` tool to solve this problem.\n\n"+
		"Follow these instructions carefully:\n"+
		"1. Set the `query` argument to a clear and concise request based on the user's problem statement.\n"+
		"2. Provide the code to be analyzed using ONE of the following methods:\n"+
		"   - Use the `file_paths` argument for one or more files.\n"+
		"   - Embed a code snippet directly into the `query` argument.\n"+
		"3. Use the following text for the `systemPrompt` argument:\n\n"+
		"<system_prompt>\n%s\n</system_prompt>\n\n"+
		"<problem_statement>\n%s\n</problem_statement>", systemPrompt, problemStatement)
}

// createSearchInstructions generates instructions for gemini_search tool
func createSearchInstructions(problemStatement string) string {
	return fmt.Sprintf(`Use the 'gemini_search' tool for this research task.

Parameter selection guide:
1. query (always required): Create a focused search query from the problem statement
2. start_time + end_time (conditional): Set both when problem implies time-sensitive information
   - Format: "YYYY-MM-DDTHH:MM:SSZ"
3. Other parameters: Use defaults unless specifically needed

Example:
INPUT: "Who won Wimbledon this year?"
OUTPUT: gemini_search(
  query="Wimbledon winner 2025",
  start_time="2025-01-01T00:00:00Z",
  end_time="2025-12-31T23:59:59Z"
)

Problem statement:
%s`, problemStatement)
}

// promptHandler is the generic handler for all prompts
func (s *GeminiServer) promptHandler(p *PromptDefinition) server.PromptHandlerFunc {
	return func(ctx context.Context, req mcp.GetPromptRequest) (*mcp.GetPromptResult, error) {
		problemStatement, ok := req.Params.Arguments["problem_statement"]
		if !ok || problemStatement == "" {
			return nil, fmt.Errorf("problem_statement argument is required")
		}

		var instructions string
		if p.Name == "research_question" {
			instructions = createSearchInstructions(problemStatement)
		} else {
			systemPrompt := p.SystemPrompt.GetSystemPrompt()
			instructions = createTaskInstructions(problemStatement, systemPrompt)
		}

		return mcp.NewGetPromptResult(
			req.Params.Name,
			[]mcp.PromptMessage{
				mcp.NewPromptMessage(mcp.RoleAssistant, mcp.NewTextContent(instructions)),
			},
		), nil
	}
}
</file>

<file path=".gitignore">
.DS_Store
bin/**
.env
go.sum
*.log

.claude/**
.gemini/**
.vscode/**
.repomix/**

repomix.config.json
.aider*
</file>

<file path="go.mod">
module GeminiMCP

go 1.24.5

require (
	github.com/joho/godotenv v1.5.1
	github.com/mark3labs/mcp-go v0.37.0
	google.golang.org/genai v1.18.0
)

require (
	cloud.google.com/go v0.116.0 // indirect
	cloud.google.com/go/auth v0.9.3 // indirect
	cloud.google.com/go/compute/metadata v0.5.0 // indirect
	github.com/bahlo/generic-list-go v0.2.0 // indirect
	github.com/buger/jsonparser v1.1.1 // indirect
	github.com/golang/groupcache v0.0.0-20210331224755-41bb18bfe9da // indirect
	github.com/google/go-cmp v0.6.0 // indirect
	github.com/google/s2a-go v0.1.8 // indirect
	github.com/google/uuid v1.6.0 // indirect
	github.com/googleapis/enterprise-certificate-proxy v0.3.4 // indirect
	github.com/gorilla/websocket v1.5.3 // indirect
	github.com/invopop/jsonschema v0.13.0 // indirect
	github.com/mailru/easyjson v0.7.7 // indirect
	github.com/spf13/cast v1.7.1 // indirect
	github.com/wk8/go-ordered-map/v2 v2.1.8 // indirect
	github.com/yosida95/uritemplate/v3 v3.0.2 // indirect
	go.opencensus.io v0.24.0 // indirect
	golang.org/x/crypto v0.27.0 // indirect
	golang.org/x/net v0.29.0 // indirect
	golang.org/x/sys v0.25.0 // indirect
	golang.org/x/text v0.18.0 // indirect
	google.golang.org/genproto/googleapis/rpc v0.0.0-20240903143218-8af14fe29dc1 // indirect
	google.golang.org/grpc v1.66.2 // indirect
	google.golang.org/protobuf v1.34.2 // indirect
	gopkg.in/yaml.v3 v3.0.1 // indirect
)
</file>

<file path="README.md">
# Gemini MCP Server

MCP (Model Control Protocol) server integrating with Google's Gemini API.

> **Important**: This server exclusively supports **Gemini 2.5 family models** for optimal thinking mode and implicit caching capabilities.

## Key Advantages

- **Single Self-Contained Binary**: Written in Go, the project compiles to a single binary with no dependencies, eliminating package manager issues and preventing unexpected changes without user knowledge
- **Dynamic Model Access**: Automatically fetches the latest available Gemini models at startup
- **Advanced Context Handling**: Efficient caching system with TTL control for repeated queries
- **Enhanced File Handling**: Seamless file integration with intelligent MIME detection
- **Production Reliability**: Robust error handling, automatic retries, and graceful degradation
- **Comprehensive Capabilities**: Full support for code analysis, general queries, and search with grounding

## Installation and Configuration

### Prerequisites

- Google Gemini API key

### Building from Source

```bash
## Clone and build
git clone https://github.com/chew-z/GeminiMCP
cd GeminiMCP
go build -o ./bin/mcp-gemini .

## Start server with environment variables
export GEMINI_API_KEY=your_api_key
export GEMINI_MODEL=gemini-2.5-pro
./bin/mcp-gemini

## Or start with HTTP transport
./bin/mcp-gemini --transport=http

## Or override settings via command line
./bin/mcp-gemini --transport=http --gemini-model=gemini-2.5-flash --enable-caching=true
```

### Client Configuration

Add this server to any MCP-compatible client like Claude Desktop by adding to your client's configuration:

```json
{
    "gemini": {
        "command": "/Users/<user>/Path/to/bin/mcp-gemini",
        "env": {
            "GEMINI_API_KEY": "YOUR_GEMINI_API_KEY",
            "GEMINI_MODEL": "gemini-2.5-pro",
            "GEMINI_SEARCH_MODEL": "gemini-2.5-flash-lite",
            "GEMINI_SYSTEM_PROMPT": "You are a senior developer. Your job is to do a thorough code review of this code...",
            "GEMINI_SEARCH_SYSTEM_PROMPT": "You are a search assistant. Your job is to find the most relevant information about this topic..."
        }
    }
}
```

**Important Notes:**

- **Environment Variables**: For Claude Desktop app all configuration variables must be included in the MCP configuration JSON shown above (in the `env` section), not as system environment variables or in .env files. Variables set outside the config JSON will not take effect for the client application.

- **Claude Desktop Config Location**:

    - On macOS: `~/Library/Application\ Support/Claude/claude_desktop_config.json`
    - On Windows: `%APPDATA%\Claude\claude_desktop_config.json`

- **Configuration Help**: If you encounter any issues configuring the Claude desktop app, refer to the [MCP Quickstart Guide](https://modelcontextprotocol.io/quickstart/user) for additional assistance.

### Command-Line Options

The server accepts several command-line flags to override environment variables:

```bash
./bin/mcp-gemini [OPTIONS]

Options:
  --gemini-model string          Gemini model name (overrides GEMINI_MODEL)
  --gemini-system-prompt string  System prompt (overrides GEMINI_SYSTEM_PROMPT)  
  --gemini-temperature float     Temperature 0.0-1.0 (overrides GEMINI_TEMPERATURE)
  --enable-caching              Enable caching (overrides GEMINI_ENABLE_CACHING)
  --enable-thinking             Enable thinking mode (overrides GEMINI_ENABLE_THINKING)
  --transport string            Transport: 'stdio' (default) or 'http'
  --auth-enabled                Enable JWT authentication for HTTP transport
  --generate-token              Generate a JWT token and exit
  --token-username string       Username for token generation (default: "admin")
  --token-role string           Role for token generation (default: "admin")
  --token-expiration int        Token expiration in hours (default: 744 = 31 days)
  --help                        Show help information
```

**Transport Modes:**
- **stdio** (default): For MCP clients like Claude Desktop that communicate via stdin/stdout
- **http**: Enables REST API endpoints for web applications or direct HTTP access

### Authentication (HTTP Transport Only)

The server supports JWT-based authentication for HTTP transport:

```bash
# Enable authentication
export GEMINI_AUTH_ENABLED=true
export GEMINI_AUTH_SECRET_KEY="your-secret-key-at-least-32-characters"

# Start server with authentication enabled
./bin/mcp-gemini --transport=http --auth-enabled=true

# Generate authentication tokens (31 days expiration by default)
./bin/mcp-gemini --generate-token --token-username=admin --token-role=admin
```

**Using Authentication:**
```bash
# Include JWT token in requests
curl -X POST http://localhost:8081/mcp \
  -H "Authorization: Bearer your-jwt-token-here" \
  -H "Content-Type: application/json" \
  -d '{"jsonrpc": "2.0", "id": 1, "method": "tools/list"}'
```

## Using this MCP server from Claude Desktop app

You can use Gemini tools directly from an LLM console by creating prompt examples that invoke the tools. Here are some example prompts for different use cases:

### Listing Available Models

Say to your LLM:

> _Please use the gemini_models tool to show me the list of available Gemini models._

The LLM will invoke the **`gemini_models`** tool and return the list of available models, organized by preference and capability. The output prioritizes recommended models for specific tasks, then organizes remaining models by version (newest to oldest).

### Code Analysis with **`gemini_ask`**

Say to your LLM:

> _Use the **`gemini_ask`** tool to analyze this Go code for potential concurrency issues:_
>
> ```
> func processItems(items []string) {
>     var wg sync.WaitGroup
>     results := make([]string, len(items))
>
>     for i, item := range items {
>         wg.Add(1)
>         go func(i int, item string) {
>             results[i] = processItem(item)
>             wg.Done()
>         }(i, item)
>     }
>
>     wg.Wait()
>     return results
> }
> ```
>
> _Please use a system prompt that focuses on code review and performance optimization._

### Creative Writing with **`gemini_ask`**

Say to your LLM:

> _Use the **`gemini_ask`** tool to create a short story about a space explorer discovering a new planet. Set a custom system prompt that encourages creative, descriptive writing with vivid imagery._

### Factual Research with **`gemini_search`**

Say to your LLM:

> _Use the **`gemini_search`** tool to find the latest information about advancements in fusion energy research from the past year. Set the start_time to one year ago and end_time to today. Include sources in your response._

### Complex Reasoning with Thinking Mode

Say to your LLM:

> _Use the `gemini_ask` tool with a thinking-capable model to solve this algorithmic problem:_
>
> _"Given an array of integers, find the longest consecutive sequence of integers. For example, given [100, 4, 200, 1, 3, 2], the longest consecutive sequence is [1, 2, 3, 4], so return 4."_
>
> _Enable thinking mode with a high budget level so I can see the detailed step-by-step reasoning process._

This will show both the final answer and the model's comprehensive reasoning process with maximum detail.

### Simple Project Analysis with Caching

Say to your LLM:

> _Please use a caching-enabled **Gemini model** to analyze our project files. Include the main.go, config.go and models.go files and ask Gemini a series of questions about our project architecture and how it could be improved. Use appropriate system prompts for each question._

With this simple prompt, the LLM will:

- Select a caching-compatible model (with -001 suffix)
- Include the specified project files
- Enable caching automatically
- Ask multiple questions while maintaining context
- Customize system prompts for each question type

This approach makes it easy to have an extended conversation about your codebase without complex configuration.

### Combined File Attachments with Caching

For programming tasks, you can directly use the file attachments feature with caching to create a more efficient workflow:

> _Use gemini_ask with model gemini-2.0-flash-001 to analyze these Go files. Please add both structs.go and models.go to the context, enable caching with a 30-minute TTL, and ask about how the model management system works in this application._
> _Use gemini_ask with model `gemini-2.5-flash` to analyze these Go files. Please add both structs.go and models.go to the context, enable caching with a 30-minute TTL, and ask about how the model management system works in this application._

The server has special optimizations for this use case, particularly useful when:
- Working with complex codebases requiring multiple files for context
- Planning to ask follow-up questions about the same code
- Debugging issues that require file context
- Code review scenarios discussing implementation details

When combining file attachments with caching, files are analyzed once and stored in the cache, making subsequent queries much faster and more cost-effective.

### Managing Multiple Caches and Reducing Costs

During a conversation, you can create and use multiple caches for different sets of files or contexts:

> _Please create a new **cache** for our frontend code (App.js, components/_.js) and analyze it separately from our backend code cache we created earlier.\*

The LLM can intelligently manage these different caches, switching between them as needed based on your queries. This capability is particularly valuable for projects with distinct components that require different analysis approaches.

**Cost Savings**: Using caching significantly reduces API costs, especially when working with large codebases or having extended conversations. By caching the context:

- Files are processed and tokenized only once instead of with every query
- Follow-up questions reuse the existing context instead of creating new API requests
- Complex analyses can be performed incrementally without re-uploading files
- Multi-session analysis becomes more economical, with some users reporting 40-60% cost reductions for extended code reviews

### Customizing System Prompts

The **`gemini_ask`** and **`gemini_search`** tools are highly versatile and not limited to programming-related queries. You can customize the system prompt for various use cases:

- **Educational content**: _"You are an expert teacher who explains complex concepts in simple terms..."_
- **Creative writing**: _"You are a creative writer specializing in vivid, engaging narratives..."_
- **Technical documentation**: _"You are a technical writer creating clear, structured documentation..."_
- **Data analysis**: _"You are a data scientist analyzing patterns and trends in information..."_

When using these tools from an LLM console, always encourage the LLM to set appropriate system prompts and parameters for the specific use case. The flexibility of system prompts allows these tools to be effective for virtually any type of query.

## Detailed Documentation

### Available Tools

The server provides three primary tools:

#### 1. **`gemini_ask`**

For code analysis, general queries, and creative tasks with optional file context.

```json
{
    "name": "gemini_ask",
    "arguments": {
        "query": "Review this Go code for concurrency issues...",
        "model": "gemini-2.5-flash",
        "systemPrompt": "You are a senior Go developer. Focus on concurrency patterns, potential race conditions, and performance implications.",
        "file_paths": ["main.go", "config.go"],
        "use_cache": true,
        "cache_ttl": "1h"
    }
}
```

Simple code analysis with file attachments:

```json
{
    "name": "gemini_ask",
    "arguments": {
        "query": "Analyze this code and suggest improvements",
        "model": "gemini-2.5-pro",
        "file_paths": ["models.go"]
    }
}
```

Combining file attachments with caching for repeated queries:

```json
{
    "name": "gemini_ask",
    "arguments": {
        "query": "Explain the main data structures in these files and how they interact",
        "model": "gemini-2.5-flash",
        "file_paths": ["models.go", "structs.go"],
        "use_cache": true,
        "cache_ttl": "30m"
    }
}
```

#### 2. **`gemini_search`**

Provides grounded answers using Google Search integration with enhanced model capabilities.

```json
{
    "name": "gemini_search",
    "arguments": {
        "query": "What is the current population of Warsaw, Poland?",
        "systemPrompt": "Optional custom search instructions",
        "enable_thinking": true,
        "thinking_budget": 8192,
        "thinking_budget_level": "medium",
        "max_tokens": 4096,
        "model": "gemini-2.5-pro",
        "start_time": "2024-01-01T00:00:00Z",
        "end_time": "2024-12-31T23:59:59Z"
    }
}
```

Returns structured responses with sources and optional thinking process:

```json
{
    "answer": "Detailed answer text based on search results...",
    "thinking": "Optional detailed reasoning process when thinking mode is enabled",
    "sources": [
        {
            "title": "Source Title",
            "url": "https://example.com/source-page",
            "type": "web"
        }
    ],
    "search_queries": ["population Warsaw Poland 2025"]
}
```

#### 3. **`gemini_models`**

Lists all available Gemini models with capabilities and caching support.

```json
{
    "name": "gemini_models",
    "arguments": {}
}
```

Returns comprehensive model information including:

- Detailed descriptions of the supported Gemini 2.5 models (Pro, Flash, Flash Lite).
- Model IDs, context window sizes, and descriptions.
- Caching capabilities (implicit and explicit).
- Usage examples
- Thinking mode support.

### Model Management

This server is optimized for and exclusively supports the **Gemini 2.5 family of models**. The `gemini_models` tool provides a detailed, static list of these supported models and their specific capabilities as presented by the server.

Key supported models (as detailed by the `gemini_models` tool):

-   **`gemini-2.5-pro`** (production):
    *   Most powerful model, 1M token context window.
    *   Best for: Complex reasoning, detailed analysis, comprehensive code review.
    *   Capabilities: Advanced thinking mode, implicit caching (2048+ token minimum), explicit caching.
-   **`gemini-2.5-flash`** (production):
    *   Balanced price-performance, 32K token context window.
    *   Best for: General programming tasks, standard code review.
    *   Capabilities: Thinking mode, implicit caching (1024+ token minimum), explicit caching.
-   **`gemini-2.5-flash-lite`** (production):
    *   Optimized for cost efficiency and low latency, 32K token context window.
    *   Best for: Search queries, lightweight tasks, quick responses.
    *   Capabilities: Thinking mode (off by default), no implicit or explicit caching.

**Always use the `gemini_models` tool to get the most current details, capabilities, and example usage for each model as presented by the server.**

### Caching System

The server offers sophisticated context caching:

- **Model Compatibility**:
    - **Gemini 2.5 Pro & Flash**: Support both implicit caching (automatic optimization by Google for repeated prefixes if content is long enough – 2048+ tokens for Pro, 1024+ for Flash) and explicit caching (user-controlled via `use_cache: true`).
    - **Gemini 2.5 Flash Lite (Preview)**: Preview versions typically do not support implicit or explicit caching.
- **Cache Control**: Set `use_cache: true` and specify `cache_ttl` (e.g., "10m", "2h")
- **File Association**: Automatically stores files and associates with cache context
- **Performance Optimization**: Local metadata caching for quick lookups

Example with caching:

```json
{
    "name": "gemini_ask",
    "arguments": {
        "query": "Follow up on our previous discussion...",
        "model": "gemini-2.5-pro",
        "use_cache": true,
        "cache_ttl": "1h"
    }
}
```

### File Handling

Robust file processing with:

- **Direct Path Integration**: Simply specify local file paths in `file_paths` array
- **Automatic Validation**: Size checking, MIME type detection, and content validation
- **Wide Format Support**: Handles common code, text, and document formats
- **Metadata Caching**: Stores file information for quick future reference

### Advanced Features

#### Thinking Mode

The server supports "thinking mode" for all compatible Gemini 2.5 models (Pro, Flash, and Flash Lite, though it's off by default for Flash Lite):

- **Model Compatibility**: Automatically validates thinking capability based on requested model
- **Tool Support**: Available in both `gemini_ask` and `gemini_search` tools
- **Configurable Budget**: Control thinking depth with budget levels or explicit token counts

Example with thinking mode:

```json
{
    "name": "gemini_ask",
    "arguments": {
        "query": "Analyze the algorithmic complexity of merge sort vs. quick sort",
        "model": "gemini-2.5-pro",
        "enable_thinking": true,
        "thinking_budget_level": "high"
    }
}
```

##### Thinking Budget Control

Configure the depth and detail of the model's thinking process:

- **Predefined Budget Levels**:

    - `none`: 0 tokens (thinking disabled)
    - `low`: 4096 tokens (default, quick analysis)
    - `medium`: 16384 tokens (detailed reasoning)
    - `high`: 24576 tokens (maximum depth for complex problems)

- **Custom Token Budget**: Alternatively, set a specific token count with `thinking_budget` parameter (0-24576)

Examples:

```json
// Using predefined level
{
  "name": "gemini_ask",
  "arguments": {
    "query": "Analyze this algorithm...",
    "model": "gemini-2.5-pro",
    "enable_thinking": true,
    "thinking_budget_level": "medium"
  }
}

// Using explicit token count
{
  "name": "gemini_search",
  "arguments": {
    "query": "Research quantum computing developments...",
    "model": "gemini-2.5-pro", // Or gemini-2.5-flash / gemini-2.5-flash-lite
    "enable_thinking": true,
    "thinking_budget": 12000
  }
}
```

#### Context Window Size Management

The server intelligently manages token limits:

- **Custom Sizing**: Set `max_tokens` parameter to control response length
- **Model-Aware Defaults**: Automatically sets appropriate defaults based on model capabilities
- **Capacity Warnings**: Provides warnings when requested tokens exceed model limits
- **Proportional Defaults**: Uses percentage-based defaults (75% for general queries, 50% for search)

Example with context window size management:

```json
{
    "name": "gemini_ask",
    "arguments": {
        "query": "Generate a detailed analysis of this code...",
        "model": "gemini-2.5-pro",
        "max_tokens": 8192
    }
}
```

### Configuration Options

#### Essential Environment Variables

| Variable                      | Description                          | Default                  |
| ----------------------------- | ------------------------------------ | ------------------------ |
| `GEMINI_API_KEY`              | Google Gemini API key                | _Required_               |
| `GEMINI_MODEL`                | Default model for `gemini_ask`       | `gemini-2.5-pro`         |
| `GEMINI_SEARCH_MODEL`         | Default model for `gemini_search`    | `gemini-2.5-flash-lite`  |
| `GEMINI_SYSTEM_PROMPT`        | System prompt for general queries    | _Custom review prompt_   |
| `GEMINI_SEARCH_SYSTEM_PROMPT` | System prompt for search             | _Custom search prompt_   |
| `GEMINI_MAX_FILE_SIZE`        | Max upload size (bytes)              | `10485760` (10MB)        |
| `GEMINI_ALLOWED_FILE_TYPES`   | Comma-separated MIME types           | [Common text/code types] |

#### Optimization Variables

| Variable                       | Description                                          | Default |
| ------------------------------ | ---------------------------------------------------- | ------- |
| `GEMINI_TIMEOUT`               | API timeout in seconds                               | `90`    |
| `GEMINI_MAX_RETRIES`           | Max API retries                                      | `2`     |
| `GEMINI_TEMPERATURE`           | Model temperature (0.0-1.0)                          | `0.4`   |
| `GEMINI_ENABLE_CACHING`        | Enable context caching                               | `true`  |
| `GEMINI_DEFAULT_CACHE_TTL`     | Default cache time-to-live                           | `1h`    |
| `GEMINI_ENABLE_THINKING`       | Enable thinking mode capability                      | `true`  |
| `GEMINI_THINKING_BUDGET_LEVEL` | Default thinking budget level (none/low/medium/high) | `low`   |
| `GEMINI_THINKING_BUDGET`       | Explicit thinking token budget (0-24576)             | `4096`  |

### Operational Features

- **Degraded Mode**: Automatically enters safe mode on initialization errors
- **Retry Logic**: Configurable exponential backoff for reliable API communication
- **Structured Logging**: Comprehensive event logging with severity levels
- **File Validation**: Secure handling with size and type restrictions

## Development

### Running Tests

```bash
go test -v
```

### Running Linter

```bash
./run_lint.sh
```

### Formatting Code

```bash
./run_format.sh
```

## Recent Changes

- **Exclusive Gemini 2.5 Support**: Server now exclusively supports the Gemini 2.5 family of models (Pro, Flash, Flash Lite) for optimal performance and access to the latest features.
- **Streamlined Model Information**: The `gemini_models` tool provides detailed, up-to-date information on supported Gemini 2.5 models, their context windows, and specific capabilities like caching and thinking mode.
- **Enhanced Caching for Gemini 2.5**: Leverages implicit caching (automatic for Pro/Flash with sufficient context) and provides robust explicit caching for Gemini 2.5 Pro and Flash models.
- **Time Range Filtering**: Added `start_time` and `end_time` to `gemini_search` for filtering results by publication date.

## License

[MIT License](LICENSE)

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the project
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request
</file>

<file path="main.go">
package main

import (
	"context"
	"flag"
	"fmt"
	"os"

	_ "github.com/joho/godotenv/autoload"
	"github.com/mark3labs/mcp-go/server"
)

// main is the entry point for the application.
// It sets up the MCP server with the appropriate handlers and starts it.
func main() {
	// Define command-line flags for configuration override
	geminiModelFlag := flag.String("gemini-model", "", "Gemini model name (overrides env var)")
	geminiSystemPromptFlag := flag.String("gemini-system-prompt", "", "System prompt (overrides env var)")
	geminiTemperatureFlag := flag.Float64("gemini-temperature", -1, "Temperature setting (0.0-1.0, overrides env var)")
	enableCachingFlag := flag.Bool("enable-caching", true, "Enable caching feature (overrides env var)")
	enableThinkingFlag := flag.Bool("enable-thinking", true, "Enable thinking mode for supported models (overrides env var)")
	transportFlag := flag.String("transport", "stdio", "Transport mode: 'stdio' (default) or 'http'")

	// Authentication flags
	authEnabledFlag := flag.Bool("auth-enabled", false, "Enable JWT authentication for HTTP transport (overrides env var)")
	generateTokenFlag := flag.Bool("generate-token", false, "Generate a JWT token and exit")
	tokenUserIDFlag := flag.String("token-user-id", "user1", "User ID for token generation")
	tokenUsernameFlag := flag.String("token-username", "admin", "Username for token generation")
	tokenRoleFlag := flag.String("token-role", "admin", "Role for token generation")
	tokenExpirationFlag := flag.Int("token-expiration", 744, "Token expiration in hours (default: 744 = 31 days)")

	flag.Parse()

	// Handle token generation if requested
	if *generateTokenFlag {
		secretKey := os.Getenv("GEMINI_AUTH_SECRET_KEY")
		CreateTokenCommand(secretKey, *tokenUserIDFlag, *tokenUsernameFlag, *tokenRoleFlag, *tokenExpirationFlag)
		return
	}

	// Create application context with logger
	logger := NewLogger(LevelInfo)
	ctx := context.WithValue(context.Background(), loggerKey, logger)

	// Create configuration from environment variables
	config, err := NewConfig()
	if err != nil {
		handleStartupError(ctx, err)
		return
	}

	// Store config in context for error handler to access
	ctx = context.WithValue(ctx, configKey, config)

	// Fetch available Gemini models first if API key is available
	// This ensures we have the latest models before validation
	if config.GeminiAPIKey != "" {
		logger.Info("Attempting to fetch available Gemini models...")
		if err := FetchGeminiModels(ctx, config.GeminiAPIKey); err != nil {
			// Just log the error but continue with fallback models
			logger.Warn("Could not fetch Gemini models: %v. Using fallback model list.", err)
		}
	} else {
		logger.Warn("No Gemini API key available, using fallback model list")
	}

	// Override with command-line flags if provided
	if *geminiModelFlag != "" {
		// We'll use the model specified, even if it's not in our known list
		// This allows for new models and preview versions
		if err := ValidateModelID(*geminiModelFlag); err != nil {
			// Just log a warning, we'll still use the model
			logger.Info("Using custom model: %s (not in known list, but may be valid)", *geminiModelFlag)
		} else {
			logger.Info("Using known model: %s", *geminiModelFlag)
		}
		config.GeminiModel = *geminiModelFlag
	}
	if *geminiSystemPromptFlag != "" {
		logger.Info("Overriding Gemini system prompt with flag value")
		config.GeminiSystemPrompt = *geminiSystemPromptFlag
	}

	// Override temperature if provided and valid
	if *geminiTemperatureFlag >= 0 {
		// Validate temperature is within range
		if *geminiTemperatureFlag > 1.0 {
			logger.Error("Invalid temperature value: %v. Must be between 0.0 and 1.0", *geminiTemperatureFlag)
			handleStartupError(ctx, fmt.Errorf("invalid temperature: %v", *geminiTemperatureFlag))
			return
		}
		logger.Info("Overriding Gemini temperature with flag value: %v", *geminiTemperatureFlag)
		config.GeminiTemperature = *geminiTemperatureFlag
	}

	// Override enable caching if flag is provided
	config.EnableCaching = *enableCachingFlag
	logger.Info("Caching feature is %s", getCachingStatusStr(config.EnableCaching))

	// Override enable thinking if flag is provided
	config.EnableThinking = *enableThinkingFlag
	logger.Info("Thinking feature is %s", getCachingStatusStr(config.EnableThinking))

	// Override authentication if flag is provided
	if *authEnabledFlag {
		config.AuthEnabled = true
		logger.Info("Authentication feature enabled via command line flag")
	}

	// Store config in context for error handler to access (already done earlier)

	// Create MCP server
	mcpServer := server.NewMCPServer(
		"gemini",
		"1.0.0",
	)

	// Create and register the Gemini server tools
	if err := setupGeminiServer(ctx, mcpServer, config); err != nil {
		handleStartupError(ctx, err)
		return
	}

	// Validate transport flag
	if *transportFlag != "stdio" && *transportFlag != "http" {
		logger.Error("Invalid transport mode: %s. Must be 'stdio' or 'http'", *transportFlag)
		os.Exit(1)
	}

	// Start the appropriate transport based on command-line flag
	if *transportFlag == "http" {
		logger.Info("Starting Gemini MCP server with HTTP transport on %s%s", config.HTTPAddress, config.HTTPPath)
		if err := startHTTPServer(ctx, mcpServer, config, logger); err != nil {
			logger.Error("HTTP server error: %v", err)
			os.Exit(1)
		}
	} else {
		logger.Info("Starting Gemini MCP server with stdio transport")
		if err := server.ServeStdio(mcpServer); err != nil {
			logger.Error("Server error: %v", err)
			os.Exit(1)
		}
	}
}

// Helper function to get caching status as a string
func getCachingStatusStr(enabled bool) string {
	if enabled {
		return "enabled"
	}
	return "disabled"
}
</file>

<file path="structs.go">
package main

import (
	"context"
	"fmt"
	"sync"
	"time"

	"github.com/mark3labs/mcp-go/mcp"
	"google.golang.org/genai"
)

// PromptDefinition defines the structure for a prompt with its system prompt
type PromptDefinition struct {
	*mcp.Prompt
	SystemPrompt SystemPromptProvider
}

// SystemPromptProvider is an interface for providing system prompts
type SystemPromptProvider interface {
	GetSystemPrompt() string
}

// StaticSystemPrompt provides a fixed system prompt
type StaticSystemPrompt string

// GetSystemPrompt returns the system prompt
func (s StaticSystemPrompt) GetSystemPrompt() string {
	return string(s)
}

// NewPromptDefinition creates a new prompt definition
func NewPromptDefinition(name, description string, systemPrompt string) *PromptDefinition {
	return &PromptDefinition{
		Prompt: &mcp.Prompt{
			Name:        name,
			Description: description,
			Arguments: []mcp.PromptArgument{
				{
					Name:        "problem_statement",
					Description: "A clear and concise description of the programming problem or task.",
					Required:    true,
				},
			},
		},
		SystemPrompt: StaticSystemPrompt(systemPrompt),
	}
}

// GeminiServer implements the ToolHandler interface for Gemini API interactions
type GeminiServer struct {
	config     *Config
	client     *genai.Client
	fileStore  *FileStore
	cacheStore *CacheStore
}

// SearchResponse is the JSON response format for the gemini_search tool
type SearchResponse struct {
	Answer        string       `json:"answer"`
	Sources       []SourceInfo `json:"sources,omitempty"`
	SearchQueries []string     `json:"search_queries,omitempty"`
}

// SourceInfo represents a source from search results
type SourceInfo struct {
	Title string `json:"title"`
	URL   string `json:"url"`
	Type  string `json:"type"` // "web" or "retrieved_context"
}

// Config holds all configuration parameters for the application
type Config struct {
	// Gemini API settings
	GeminiAPIKey             string
	GeminiModel              string // Default model for 'gemini_ask'
	GeminiSearchModel        string // Default model for 'gemini_search'
	GeminiSystemPrompt       string
	GeminiSearchSystemPrompt string
	GeminiTemperature        float64

	// HTTP client settings
	HTTPTimeout time.Duration

	// HTTP transport settings
	EnableHTTP      bool          // Enable HTTP transport
	HTTPAddress     string        // Server address (default: ":8080")
	HTTPPath        string        // Base path (default: "/mcp")
	HTTPStateless   bool          // Stateless mode
	HTTPHeartbeat   time.Duration // Heartbeat interval
	HTTPCORSEnabled bool          // Enable CORS
	HTTPCORSOrigins []string      // Allowed origins

	// Authentication settings
	AuthEnabled   bool   // Enable JWT authentication for HTTP transport
	AuthSecretKey string // Secret key for JWT signing and verification

	// Retry settings
	MaxRetries     int
	InitialBackoff time.Duration
	MaxBackoff     time.Duration

	// File handling settings
	MaxFileSize      int64    // Max file size in bytes
	AllowedFileTypes []string // Allowed MIME types

	// Cache settings
	EnableCaching   bool          // Enable/disable caching
	DefaultCacheTTL time.Duration // Default TTL if not specified

	// Thinking settings
	EnableThinking      bool   // Enable/disable thinking mode for supported models
	ThinkingBudget      int    // Maximum number of tokens to allocate for thinking
	ThinkingBudgetLevel string // Thinking budget level (none, low, medium, high)

	// Prompt defaults
	ProjectLanguage         string // Default language for code analysis (e.g., "go", "python")
	PromptDefaultAudience   string // Default audience level for code explanations (beginner, intermediate, expert)
	PromptDefaultFocus      string // Default focus areas for analysis (security, performance, style, etc.)
	PromptDefaultSeverity   string // Default minimum severity level for issues (info, warning, error)
	PromptDefaultDocFormat  string // Default documentation format (markdown, rst, plain_text)
	PromptDefaultFramework  string // Default testing framework (standard, jest, pytest, etc.)
	PromptDefaultCoverage   string // Default test coverage level (basic, comprehensive)
	PromptDefaultCompliance string // Default compliance standards (OWASP, NIST, etc.)
}

// CacheRequest represents a request to create a cached context
type CacheRequest struct {
	Model        string   `json:"model"`
	SystemPrompt string   `json:"system_prompt,omitempty"`
	FileIDs      []string `json:"file_ids,omitempty"`
	Content      string   `json:"content,omitempty"`
	TTL          string   `json:"ttl,omitempty"` // Duration like "1h", "24h", etc.
	DisplayName  string   `json:"display_name,omitempty"`
}

// CacheInfo represents information about a cached context
type CacheInfo struct {
	ID          string    `json:"id"`   // The unique ID (last part of the Name)
	Name        string    `json:"name"` // The full resource name
	DisplayName string    `json:"display_name"`
	Model       string    `json:"model"`
	CreatedAt   time.Time `json:"created_at"`
	ExpiresAt   time.Time `json:"expires_at"`
	FileIDs     []string  `json:"file_ids,omitempty"`
}

// CacheStore manages cache metadata
type CacheStore struct {
	client    *genai.Client
	config    *Config
	fileStore *FileStore
	mu        sync.RWMutex
	cacheInfo map[string]*CacheInfo // Map of ID -> CacheInfo
}

// ModelVersion represents an actual API-addressable Gemini model
type ModelVersion struct {
	ID              string `json:"id"`               // The version ID used by the API (e.g., "gemini-2.5-pro-exp-03-25")
	Name            string `json:"name"`             // Human-readable name
	SupportsCaching bool   `json:"supports_caching"` // Whether this version supports caching
	IsPreferred     bool   `json:"is_preferred"`     // Whether this is the preferred version of the model family
}

// GeminiModelInfo represents a family of related models
type GeminiModelInfo struct {
	FamilyID             string         `json:"family_id"`              // Model family identifier (e.g., "gemini-2.5-pro")
	Name                 string         `json:"name"`                   // Human-readable family name
	Description          string         `json:"description"`            // Description of the model family
	SupportsThinking     bool           `json:"supports_thinking"`      // Whether this model family supports thinking mode
	ContextWindowSize    int            `json:"context_window_size"`    // Maximum context window size in tokens
	PreferredForThinking bool           `json:"preferred_for_thinking"` // Whether this family is preferred for thinking tasks
	PreferredForCaching  bool           `json:"preferred_for_caching"`  // Whether this family is preferred for caching tasks
	PreferredForSearch   bool           `json:"preferred_for_search"`   // Whether this family is preferred for search tasks
	Versions             []ModelVersion `json:"versions"`               // Available versions of this model family
}

// FileUploadRequest represents a request to upload a file
type FileUploadRequest struct {
	FileName    string `json:"filename"`
	MimeType    string `json:"mime_type"`
	Content     []byte `json:"content"`
	DisplayName string `json:"display_name,omitempty"`
}

// FileInfo represents information about a stored file
type FileInfo struct {
	ID          string    `json:"id"`           // The unique ID (last part of the Name)
	Name        string    `json:"name"`         // The full resource name (e.g., "files/abc123")
	URI         string    `json:"uri"`          // The URI to use in requests
	DisplayName string    `json:"display_name"` // Human-readable name
	MimeType    string    `json:"mime_type"`
	Size        int64     `json:"size"`
	UploadedAt  time.Time `json:"uploaded_at"`
	ExpiresAt   time.Time `json:"expires_at"`
}

// FileStore manages file metadata
type FileStore struct {
	client   *genai.Client
	config   *Config
	mu       sync.RWMutex
	fileInfo map[string]*FileInfo // Map of ID -> FileInfo
}

// GeminiServer implements the ToolHandler interface to provide research capabilities
// through Google's Gemini API.
// Defined in gemini.go

// ErrorGeminiServer implements the ToolHandler interface but returns error responses
// for all calls. Used when the Gemini server is in degraded mode due to initialization errors.
type ErrorGeminiServer struct {
	errorMessage string
	config       *Config // Added to check EnableCaching
}

// handleErrorResponse is a handler function that can be used with mark3labs/mcp-go's AddTool
func (s *ErrorGeminiServer) handleErrorResponse(ctx context.Context, req mcp.CallToolRequest) (*mcp.CallToolResult, error) {
	// Get logger from context
	logger := getLoggerFromContext(ctx)

	// Log which tool was attempted
	toolName := req.Params.Name
	logger.Info("Tool '%s' called in error mode", toolName)

	// Return an error result with the initialization error message
	// Include the tool name for better debugging
	errorMessage := fmt.Sprintf("Error in tool '%s': %s", toolName, s.errorMessage)
	return createErrorResult(errorMessage), nil
}
</file>

<file path="direct_handlers.go">
package main

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"os"
	"path/filepath"
	"strings"
	"time"

	"github.com/mark3labs/mcp-go/mcp"
	"google.golang.org/genai"
)

// GeminiAskHandler is a handler for the gemini_ask tool that uses mcp-go types directly
func (s *GeminiServer) GeminiAskHandler(ctx context.Context, req mcp.CallToolRequest) (*mcp.CallToolResult, error) {
	logger := getLoggerFromContext(ctx)
	logger.Info("Handling gemini_ask request with direct handler")

	// Extract and validate query parameter (required)
	query, ok := req.GetArguments()["query"].(string)
	if !ok || query == "" {
		return createErrorResult("query must be a string and cannot be empty"), nil
	}

	// Create Gemini model configuration
	config, modelName, err := createModelConfig(ctx, req, s.config, s.config.GeminiModel)
	if err != nil {
		return createErrorResult(fmt.Sprintf("Error creating model configuration: %v", err)), nil
	}

	// Extract file paths if provided
	filePaths := extractArgumentStringArray(req, "file_paths")

	// Check if caching is requested
	useCache := extractArgumentBool(req, "use_cache", false)
	cacheTTL := extractArgumentString(req, "cache_ttl", "")

	// Check if thinking mode is requested (used to determine if caching should be used)
	enableThinking := extractArgumentBool(req, "enable_thinking", s.config.EnableThinking)

	// Caching and thinking conflict - prioritize thinking if both are requested
	if useCache && enableThinking {
		logger.Warn("Both caching and thinking mode were requested - prioritizing thinking mode")
		useCache = false
	}

	// Handle caching if enabled and supported by the model
	var cacheID string
	var cacheErr error

	if useCache && s.config.EnableCaching {
		// Get model information to check if it supports caching
		modelVersion := GetModelVersion(modelName)
		if modelVersion != nil && modelVersion.SupportsCaching {
			// Try to create cache from files if provided
			cacheID, cacheErr = s.createCacheFromFiles(ctx, query, modelName, filePaths, cacheTTL,
				extractArgumentString(req, "systemPrompt", s.config.GeminiSystemPrompt))

			if cacheErr != nil {
				logger.Warn("Failed to create cache, falling back to regular request: %v", cacheErr)
			} else if cacheID != "" {
				logger.Info("Using cache with ID: %s", cacheID)
				return s.handleQueryWithCacheDirect(ctx, cacheID, query)
			}
		} else {
			logger.Warn("Model %s does not support caching, falling back to regular request", modelName)
		}
	}

	// Validate client and models before proceeding
	if s.client == nil || s.client.Models == nil {
		logger.Error("Gemini client or Models service not properly initialized")
		return createErrorResult("Internal error: Gemini client not properly initialized"), nil
	}

	// Process with files if provided
	if len(filePaths) > 0 {
		return s.processWithFiles(ctx, query, filePaths, modelName, config, cacheErr)
	} else {
		return s.processWithoutFiles(ctx, query, modelName, config, cacheErr)
	}
}

// createCacheFromFiles creates a cache from the provided files and returns the cache ID
func (s *GeminiServer) createCacheFromFiles(ctx context.Context, query, modelName string,
	filePaths []string, cacheTTL, systemPrompt string) (string, error) {

	logger := getLoggerFromContext(ctx)

	// Check if file store is properly initialized
	if s.fileStore == nil {
		return "", fmt.Errorf("FileStore not properly initialized")
	}

	// Create a list of file IDs from uploaded files
	var fileIDs []string

	// Upload each file to the API
	for _, filePath := range filePaths {
		// Read the file
		content, err := os.ReadFile(filePath)
		if err != nil {
			logger.Error("Failed to read file %s: %v", filePath, err)
			continue
		}

		// Get mime type and filename
		mimeType := getMimeTypeFromPath(filePath)
		fileName := filepath.Base(filePath)

		// Create upload request
		uploadReq := &FileUploadRequest{
			FileName:    fileName,
			MimeType:    mimeType,
			Content:     content,
			DisplayName: fileName,
		}

		// Upload the file
		fileInfo, err := s.fileStore.UploadFile(ctx, uploadReq)
		if err != nil {
			logger.Error("Failed to upload file %s: %v", filePath, err)
			continue
		}

		logger.Info("Successfully uploaded file %s with ID: %s for caching", fileName, fileInfo.ID)
		fileIDs = append(fileIDs, fileInfo.ID)
	}

	// If no files were uploaded successfully, return error
	if len(fileIDs) == 0 && len(filePaths) > 0 {
		return "", fmt.Errorf("failed to upload any files for caching")
	}

	// Create cache request
	cacheReq := &CacheRequest{
		Model:        modelName,
		SystemPrompt: systemPrompt,
		FileIDs:      fileIDs,
		TTL:          cacheTTL,
		Content:      query,
	}

	// Create the cache
	cacheInfo, err := s.cacheStore.CreateCache(ctx, cacheReq)
	if err != nil {
		return "", fmt.Errorf("failed to create cache: %w", err)
	}

	return cacheInfo.ID, nil
}

// handleQueryWithCacheDirect handles a query with a previously created cache
func (s *GeminiServer) handleQueryWithCacheDirect(ctx context.Context, cacheID, query string) (*mcp.CallToolResult, error) {
	logger := getLoggerFromContext(ctx)

	// Get the cache
	cacheInfo, err := s.cacheStore.GetCache(ctx, cacheID)
	if err != nil {
		logger.Error("Failed to get cache with ID %s: %v", cacheID, err)
		return createErrorResult(fmt.Sprintf("Failed to get cache: %v", err)), nil
	}

	// Use the cached content for the query
	contents := []*genai.Content{
		genai.NewContentFromText(query, genai.RoleUser),
	}

	// Create the configuration
	config := &genai.GenerateContentConfig{
		CachedContent: cacheInfo.Name,
	}

	// Make the request to the API
	response, err := s.client.Models.GenerateContent(ctx, cacheInfo.Model, contents, config)
	if err != nil {
		logger.Error("Failed to generate content with cached content: %v", err)
		return createErrorResult(fmt.Sprintf("Error from Gemini API: %v", err)), nil
	}

	// Convert to MCP result
	return convertGenaiResponseToMCPResult(response), nil
}

// processWithFiles handles a Gemini API request with file attachments
func (s *GeminiServer) processWithFiles(ctx context.Context, query string, filePaths []string,
	modelName string, config *genai.GenerateContentConfig, cacheErr error) (*mcp.CallToolResult, error) {

	logger := getLoggerFromContext(ctx)

	// Create initial content with the query
	contents := []*genai.Content{
		genai.NewContentFromText(query, genai.RoleUser),
	}

	// Process each file
	for _, filePath := range filePaths {
		// Read the file
		content, err := os.ReadFile(filePath)
		if err != nil {
			logger.Error("Failed to read file %s: %v", filePath, err)
			continue
		}

		// Get mime type and filename
		mimeType := getMimeTypeFromPath(filePath)
		fileName := filepath.Base(filePath)

		// Upload the file to Gemini
		logger.Info("Uploading file %s with mime type %s", fileName, mimeType)
		uploadConfig := &genai.UploadFileConfig{
			MIMEType:    mimeType,
			DisplayName: fileName,
		}

		file, err := s.client.Files.Upload(ctx, bytes.NewReader(content), uploadConfig)
		if err != nil {
			logger.Error("Failed to upload file %s: %v - falling back to direct content", filePath, err)
			// Fallback to direct content if upload fails
			contents = append(contents, genai.NewContentFromText(string(content), genai.RoleUser))
			continue
		}

		// Add file to contents using the URI
		contents = append(contents, genai.NewContentFromURI(file.URI, mimeType, genai.RoleUser))
	}

	// Generate content with files
	response, err := s.client.Models.GenerateContent(ctx, modelName, contents, config)
	if err != nil {
		logger.Error("Gemini API error: %v", err)
		if cacheErr != nil {
			// If there was also a cache error, include it in the response
			return createErrorResult(fmt.Sprintf("Error from Gemini API: %v\nCache error: %v", err, cacheErr)), nil
		}
		return createErrorResult(fmt.Sprintf("Error from Gemini API: %v", err)), nil
	}

	// Convert to MCP result
	return convertGenaiResponseToMCPResult(response), nil
}

// processWithoutFiles handles a Gemini API request without file attachments
func (s *GeminiServer) processWithoutFiles(ctx context.Context, query string,
	modelName string, config *genai.GenerateContentConfig, cacheErr error) (*mcp.CallToolResult, error) {

	logger := getLoggerFromContext(ctx)

	// Create content with just the query
	contents := []*genai.Content{
		genai.NewContentFromText(query, genai.RoleUser),
	}

	// Generate content
	response, err := s.client.Models.GenerateContent(ctx, modelName, contents, config)
	if err != nil {
		logger.Error("Gemini API error: %v", err)
		if cacheErr != nil {
			// If there was also a cache error, include it in the response
			return createErrorResult(fmt.Sprintf("Error from Gemini API: %v\nCache error: %v", err, cacheErr)), nil
		}
		return createErrorResult(fmt.Sprintf("Error from Gemini API: %v", err)), nil
	}

	// Convert to MCP result
	return convertGenaiResponseToMCPResult(response), nil
}

// GeminiSearchHandler is a handler for the gemini_search tool that uses mcp-go types directly
func (s *GeminiServer) GeminiSearchHandler(ctx context.Context, req mcp.CallToolRequest) (*mcp.CallToolResult, error) {
	logger := getLoggerFromContext(ctx)
	logger.Info("Handling gemini_search request with direct handler")

	// Extract and validate query parameter (required)
	query, ok := req.GetArguments()["query"].(string)
	if !ok || query == "" {
		return createErrorResult("query must be a string and cannot be empty"), nil
	}

	// Extract optional system prompt - use search-specific prompt as default
	systemPrompt := extractArgumentString(req, "systemPrompt", s.config.GeminiSearchSystemPrompt)

	// Extract optional model parameter - use search-specific model as default
	modelName := extractArgumentString(req, "model", s.config.GeminiSearchModel)
	if err := ValidateModelID(modelName); err != nil {
		logger.Error("Invalid model requested: %v", err)
		return createErrorResult(fmt.Sprintf("Invalid model specified: %v", err)), nil
	}

	// Resolve model ID to ensure we use a valid API-addressable version
	resolvedModelID := ResolveModelID(modelName)
	if resolvedModelID != modelName {
		logger.Info("Resolved model ID from '%s' to '%s'", modelName, resolvedModelID)
		modelName = resolvedModelID
	}
	logger.Info("Using %s model for Google Search integration", modelName)

	// Get model information for context window and thinking capability
	modelInfo := GetModelByID(modelName)
	if modelInfo == nil {
		logger.Warn("Model information not found for %s, using default parameters", modelName)
	}

	// Create the generate content configuration
	googleSearch := &genai.GoogleSearch{}

	// Extract and validate time range filter parameters
	startTimeStr := extractArgumentString(req, "start_time", "")
	endTimeStr := extractArgumentString(req, "end_time", "")

	// Both must be provided if either is provided
	if (startTimeStr != "" && endTimeStr == "") || (startTimeStr == "" && endTimeStr != "") {
		return createErrorResult("Both start_time and end_time must be provided for time range filtering"), nil
	}

	// If both time parameters are provided, create the time range filter
	if startTimeStr != "" && endTimeStr != "" {
		startTime, err := time.Parse(time.RFC3339, startTimeStr)
		if err != nil {
			logger.Error("Invalid start_time format: %v", err)
			return createErrorResult(fmt.Sprintf("Invalid start_time format: %v. Must be RFC3339 format (e.g. '2024-01-01T00:00:00Z')", err)), nil
		}

		endTime, err := time.Parse(time.RFC3339, endTimeStr)
		if err != nil {
			logger.Error("Invalid end_time format: %v", err)
			return createErrorResult(fmt.Sprintf("Invalid end_time format: %v. Must be RFC3339 format (e.g. '2024-12-31T23:59:59Z')", err)), nil
		}

		// Ensure start time is before or equal to end time
		if startTime.After(endTime) {
			return createErrorResult("start_time must be before or equal to end_time"), nil
		}

		// Create the time range filter
		googleSearch.TimeRangeFilter = &genai.Interval{
			StartTime: startTime,
			EndTime:   endTime,
		}
		logger.Info("Applying time range filter from %s to %s", startTime.Format(time.RFC3339), endTime.Format(time.RFC3339))
	}

	config := &genai.GenerateContentConfig{
		SystemInstruction: genai.NewContentFromText(systemPrompt, ""),
		Temperature:       genai.Ptr(float32(s.config.GeminiTemperature)),
		Tools: []*genai.Tool{
			{
				GoogleSearch: googleSearch,
			},
		},
	}

	// Configure thinking
	enableThinking := extractArgumentBool(req, "enable_thinking", s.config.EnableThinking)
	if enableThinking && modelInfo != nil && modelInfo.SupportsThinking {
		thinkingConfig := &genai.ThinkingConfig{
			IncludeThoughts: true,
		}

		// Determine thinking budget
		thinkingBudget := 0

		// Check for level first
		args := req.GetArguments()
		if levelStr, ok := args["thinking_budget_level"].(string); ok && levelStr != "" {
			thinkingBudget = getThinkingBudgetFromLevel(levelStr)
			logger.Info("Setting thinking budget to %d tokens from level: %s", thinkingBudget, levelStr)
		} else if budgetRaw, ok := args["thinking_budget"].(float64); ok && budgetRaw >= 0 {
			// If explicit budget was provided
			thinkingBudget = int(budgetRaw)
			logger.Info("Setting thinking budget to %d tokens from explicit value", thinkingBudget)
		} else if s.config.ThinkingBudget > 0 {
			// Fall back to config value
			thinkingBudget = s.config.ThinkingBudget
			logger.Info("Using default thinking budget of %d tokens", thinkingBudget)
		}

		// Set thinking budget if greater than 0
		if thinkingBudget > 0 {
			budget := int32(thinkingBudget)
			thinkingConfig.ThinkingBudget = &budget
		}

		config.ThinkingConfig = thinkingConfig
		logger.Info("Thinking mode enabled for search request with model %s", modelName)
	} else if enableThinking {
		if modelInfo != nil {
			logger.Warn("Thinking mode requested but model %s doesn't support it", modelName)
		} else {
			logger.Warn("Thinking mode requested but unknown if model supports it")
		}
	}

	// Configure max tokens (50% of context window by default for search)
	configureMaxTokensOutput(ctx, config, req, modelInfo, 0.5)

	// Create content with the query
	contents := []*genai.Content{
		genai.NewContentFromText(query, genai.RoleUser),
	}

	// Validate client and models before proceeding
	if s.client == nil || s.client.Models == nil {
		logger.Error("Gemini client or Models service not properly initialized")
		return createErrorResult("Internal error: Gemini client not properly initialized"), nil
	}

	// Initialize response data
	responseText := ""
	var sources []SourceInfo
	var searchQueries []string

	// Track seen URLs to avoid duplicates
	seenURLs := make(map[string]bool)

	// Stream the response
	for result, err := range s.client.Models.GenerateContentStream(ctx, modelName, contents, config) {
		if err != nil {
			logger.Error("Gemini Search API error: %v", err)
			return createErrorResult(fmt.Sprintf("Error from Gemini Search API: %v", err)), nil
		}

		// Extract text and metadata from the response
		if len(result.Candidates) > 0 && result.Candidates[0].Content != nil {
			responseText += result.Text()

			// Extract metadata if available
			if metadata := result.Candidates[0].GroundingMetadata; metadata != nil {
				// Collect search queries
				if len(metadata.WebSearchQueries) > 0 && len(searchQueries) == 0 {
					searchQueries = metadata.WebSearchQueries
				}

				// Collect sources from grounding chunks
				for _, chunk := range metadata.GroundingChunks {
					var source SourceInfo

					if web := chunk.Web; web != nil {
						// Skip if we've seen this URL already
						if seenURLs[web.URI] {
							continue
						}

						source = SourceInfo{
							Title: web.Title,
							URL:   web.URI,
							Type:  "web",
						}
						seenURLs[web.URI] = true
					} else if ctx := chunk.RetrievedContext; ctx != nil {
						// Skip if we've seen this URL already
						if seenURLs[ctx.URI] {
							continue
						}

						source = SourceInfo{
							Title: ctx.Title,
							URL:   ctx.URI,
							Type:  "retrieved_context",
						}
						seenURLs[ctx.URI] = true
					}

					if source.URL != "" {
						sources = append(sources, source)
					}
				}
			}
		}
	}

	// Check for empty content and provide a fallback message
	if responseText == "" {
		responseText = `The Gemini Search model returned an empty response.
			This might indicate an issue with the search functionality or that
			no relevant information was found. Please try rephrasing your question
			or providing more specific details.`
	}

	// Create the response JSON
	response := SearchResponse{
		Answer:        responseText,
		Sources:       sources,
		SearchQueries: searchQueries,
	}

	// Convert to JSON and return as text content
	responseJSON, err := json.Marshal(response)
	if err != nil {
		logger.Error("Failed to marshal search response: %v", err)
		return createErrorResult(fmt.Sprintf("Failed to format search response: %v", err)), nil
	}

	return &mcp.CallToolResult{
		Content: []mcp.Content{
			mcp.NewTextContent(string(responseJSON)),
		},
	}, nil
}

// GeminiModelsHandler is a handler for the gemini_models tool that uses mcp-go types directly
func (s *GeminiServer) GeminiModelsHandler(ctx context.Context, req mcp.CallToolRequest) (*mcp.CallToolResult, error) {
	logger := getLoggerFromContext(ctx)
	logger.Info("Handling gemini_models request with direct handler")

	// Create formatted response using strings.Builder
	var formattedContent strings.Builder

	// Write the content using helper to make error checking cleaner
	write := func(format string, args ...interface{}) error {
		_, err := formattedContent.WriteString(fmt.Sprintf(format, args...))
		return err
	}

	// Write the header
	if err := write("# Available Gemini 2.5 Models\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("This server supports 3 Gemini 2.5 models and provides 2 main tools:\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("- `gemini_ask`: For general queries, coding problems (default: code review system prompt)\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("- `gemini_search`: For search-grounded queries (default: search assistant system prompt)\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	// Gemini 2.5 Pro
	if err := write("## Gemini 2.5 Pro\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}
	if err := write("- **Model ID**: `gemini-2.5-pro` (production)\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}
	if err := write("- **Description**: Most powerful model with maximum accuracy and performance\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}
	if err := write("- **Context Window**: 1M tokens\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}
	if err := write("- **Best for**: Complex reasoning, detailed analysis, comprehensive code review\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}
	if err := write("- **Thinking Mode**: Yes (advanced reasoning capabilities)\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}
	if err := write("- **Implicit Caching**: Yes (automatic optimization, 2048+ token minimum)\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}
	if err := write("- **Explicit Caching**: Yes (user-controlled via `use_cache`)\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	// Gemini 2.5 Flash
	if err := write("## Gemini 2.5 Flash\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}
	if err := write("- **Model ID**: `gemini-2.5-flash` (production)\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}
	if err := write("- **Description**: Balanced price-performance with fast responses\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}
	if err := write("- **Context Window**: 32K tokens\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}
	if err := write("- **Best for**: General programming tasks, standard code review\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}
	if err := write("- **Thinking Mode**: Yes\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}
	if err := write("- **Implicit Caching**: Yes (automatic optimization, 1024+ token minimum)\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}
	if err := write("- **Explicit Caching**: Yes (user-controlled via `use_cache`)\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	// Gemini 2.5 Flash Lite
	if err := write("## Gemini 2.5 Flash Lite\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}
	if err := write("- **Model ID**: `gemini-2.5-flash-lite`\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}
	if err := write("- **Description**: Optimized for cost efficiency and low latency\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}
	if err := write("- **Context Window**: 32K tokens\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}
	if err := write("- **Best for**: Search queries, lightweight tasks, quick responses\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}
	if err := write("- **Thinking Mode**: Yes (off by default for speed/cost, can be enabled)\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}
	if err := write("- **Implicit Caching**: No\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}
	if err := write("- **Explicit Caching**: No (preview limitation)\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	// Tool Usage Examples
	if err := write("## Tool Usage Examples\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	// gemini_ask examples
	if err := write("### gemini_ask Examples\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("**General Problem (non-coding):**\n```json\n{\n  \"query\": \"Explain quantum computing in simple terms\",\n  \"model\": \"gemini-2.5-flash\",\n  \"systemPrompt\": \"You are an expert science communicator. Explain complex topics clearly for a general audience.\"\n}\n```\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("**Coding Problem with Files and Cache:**\n```json\n{\n  \"query\": \"Review this code for security vulnerabilities and performance issues\",\n  \"model\": \"gemini-2.5-pro\",\n  \"file_paths\": [\"/path/to/auth.go\", \"/path/to/database.go\"],\n  \"use_cache\": true,\n  \"cache_ttl\": \"30m\",\n  \"enable_thinking\": true\n}\n```\n*Note: Default system prompt optimized for code review will be used*\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("**Custom System Prompt Override:**\n```json\n{\n  \"query\": \"Analyze this code architecture\",\n  \"model\": \"gemini-2.5-pro\",\n  \"systemPrompt\": \"You are a senior software architect. Focus on design patterns, scalability, and maintainability.\",\n  \"file_paths\": [\"/path/to/main.go\"]\n}\n```\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	// gemini_search examples
	if err := write("### gemini_search Examples\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("**Basic Search:**\n```json\n{\n  \"query\": \"What are the latest developments in Go programming language?\",\n  \"model\": \"gemini-2.5-flash-lite\"\n}\n```\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("**Search with Time Filtering:**\n```json\n{\n  \"query\": \"Recent security vulnerabilities in JavaScript frameworks\",\n  \"model\": \"gemini-2.5-flash\",\n  \"start_time\": \"2024-01-01T00:00:00Z\",\n  \"end_time\": \"2024-12-31T23:59:59Z\"\n}\n```\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("**Search with Thinking Mode (Flash Lite):**\n```json\n{\n  \"query\": \"Compare the pros and cons of different cloud deployment strategies\",\n  \"model\": \"gemini-2.5-flash-lite\",\n  \"enable_thinking\": true,\n  \"thinking_budget_level\": \"medium\"\n}\n```\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	// System Prompt Details
	if err := write("## System Prompt Details\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("**Default System Prompts:**\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("- **gemini_ask**: Optimized for thorough code review (senior developer perspective)\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("- **gemini_search**: Helpful search assistant for accurate, up-to-date information\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("**Override via:**\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("- `systemPrompt` parameter in requests\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("- `GEMINI_SYSTEM_PROMPT` env variable (for gemini_ask)\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("- `GEMINI_SEARCH_SYSTEM_PROMPT` env variable (for gemini_search)\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("- Command line flags: `--gemini-system-prompt`\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	// File Attachments
	if err := write("## File Attachments (gemini_ask only)\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("Attach files to provide context for your queries. This is particularly useful for code review, debugging, and analysis:\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("```json\n// Code review with multiple files\n{\n  \"query\": \"Review this code for potential issues and suggest improvements\",\n  \"model\": \"gemini-2.5-pro\",\n  \"file_paths\": [\n    \"/path/to/main.go\",\n    \"/path/to/utils.go\",\n    \"/path/to/config.yaml\"\n  ]\n}\n\n// Documentation analysis\n{\n  \"query\": \"Explain how these components interact and suggest documentation improvements\",\n  \"model\": \"gemini-2.5-flash\",\n  \"file_paths\": [\n    \"/path/to/README.md\",\n    \"/path/to/api.go\"\n  ]\n}\n```\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	// Caching
	if err := write("## Caching (gemini_ask only)\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("**Implicit Caching (Automatic):**\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("- 75% token discount for requests with common prefixes\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("- Pro: 2048+ tokens minimum\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("- Flash: 1024+ tokens minimum\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("- Keep content at the beginning of requests the same, add variable content at the end\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("**Explicit Caching (Manual):**\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("- Available for Pro and Flash only\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("- Use `use_cache: true` parameter\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("- Custom TTL with `cache_ttl` (default: 10 minutes)\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("```json\n// Enable explicit caching\n{\n  \"query\": \"Analyze this codebase structure\",\n  \"model\": \"gemini-2.5-flash\",\n  \"file_paths\": [\"/path/to/large/codebase\"],\n  \"use_cache\": true,\n  \"cache_ttl\": \"30m\"\n}\n```\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	// Thinking Mode
	if err := write("## Thinking Mode (both tools)\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("All Gemini 2.5 models support thinking mode, which shows the model's detailed reasoning process. Flash-Lite has thinking off by default for speed/cost optimization.\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("**Thinking Budget Levels:**\n- `none`: 0 tokens (disabled)\n- `low`: 4,096 tokens\n- `medium`: 16,384 tokens\n- `high`: 24,576 tokens (maximum)\n\nOr use `thinking_budget` to set a specific token count (0-24,576).\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("```json\n// Enable thinking with budget level\n{\n  \"query\": \"Solve this complex algorithm problem step by step\",\n  \"model\": \"gemini-2.5-pro\",\n  \"enable_thinking\": true,\n  \"thinking_budget_level\": \"high\"\n}\n\n// Custom thinking budget\n{\n  \"query\": \"Debug this complex issue\",\n  \"model\": \"gemini-2.5-pro\",\n  \"enable_thinking\": true,\n  \"thinking_budget\": 12000\n}\n```\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	// Time Filtering
	if err := write("## Time Filtering (gemini_search only)\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("Filter search results by publication date using RFC3339 format:\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("- Use `start_time` and `end_time` together (both required)\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("- Format: `YYYY-MM-DDTHH:MM:SSZ`\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	// Advanced Examples
	if err := write("## Advanced Examples\n\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	if err := write("```json\n// Comprehensive code review with thinking and caching (gemini_ask)\n{\n  \"query\": \"Perform a thorough security and performance review of this codebase\",\n  \"model\": \"gemini-2.5-pro\",\n  \"file_paths\": [\n    \"/path/to/main.go\",\n    \"/path/to/auth.go\",\n    \"/path/to/database.go\"\n  ],\n  \"enable_thinking\": true,\n  \"thinking_budget_level\": \"medium\",\n  \"use_cache\": true,\n  \"cache_ttl\": \"1h\"\n}\n\n// Custom system prompt with file context (gemini_ask)\n{\n  \"query\": \"Suggest architectural improvements for better scalability\",\n  \"model\": \"gemini-2.5-pro\",\n  \"systemPrompt\": \"You are a senior software architect. Focus on scalability, maintainability, and best practices.\",\n  \"file_paths\": [\"/path/to/architecture/overview.md\"],\n  \"enable_thinking\": true\n}\n```\n"); err != nil {
		logger.Error("Error writing to response: %v", err)
		return createErrorResult("Error generating model list"), nil
	}

	// Return the formatted content
	return &mcp.CallToolResult{
		Content: []mcp.Content{
			mcp.NewTextContent(formattedContent.String()),
		},
	}, nil
}
</file>

<file path="config.go">
package main

import (
	"errors"
	"fmt"
	"os"
	"strconv"
	"strings"
	"time"
)

// Default configuration values
const (
	// Note: if this value changes, make sure to update the models.go list
	defaultGeminiModel        = "gemini-2.5-pro"
	defaultGeminiSearchModel  = "gemini-2.5-flash-lite" // Default model specifically for search
	defaultGeminiTemperature  = 0.4
	defaultGeminiSystemPrompt = `
You are a senior developer. Your job is to do a thorough code review of this code.
You should write it up and output markdown.
Include line numbers, and contextual info.
Your code review will be passed to another teammate, so be thorough.
Think deeply  before writing the code review. Review every part, and don't hallucinate.
`
	// System prompt for search-based queries
	defaultGeminiSearchSystemPrompt = `
You are a helpful search assistant. Use the Google Search results to provide accurate and up-to-date information.
Your answers should be comprehensive but concise, focusing on the most relevant information.
Cite your sources when appropriate and maintain a neutral, informative tone.
If the search results don't contain enough information to fully answer the query, acknowledge the limitations.
`
	// File handling defaults
	defaultMaxFileSize = int64(10 * 1024 * 1024) // 10MB explicitly as int64

	// HTTP transport defaults
	defaultEnableHTTP      = false
	defaultHTTPAddress     = ":8080"
	defaultHTTPPath        = "/mcp"
	defaultHTTPStateless   = false
	defaultHTTPHeartbeat   = 0 * time.Second // No heartbeat by default
	defaultHTTPCORSEnabled = true

	// Authentication defaults
	defaultAuthEnabled = false // Authentication disabled by default

	// Cache settings defaults
	defaultEnableCaching   = true
	defaultDefaultCacheTTL = 1 * time.Hour

	// Thinking settings
	defaultEnableThinking      = true
	defaultThinkingBudgetLevel = "low" // Default thinking budget level
	thinkingBudgetNone         = 0     // None: Thinking disabled
	thinkingBudgetLow          = 4096  // Low: 4096 tokens
	thinkingBudgetMedium       = 16384 // Medium: 16384 tokens
	thinkingBudgetHigh         = 24576 // High: Maximum allowed by Gemini (24576 tokens)
)

// Config struct definition moved to structs.go

// getThinkingBudgetFromLevel converts a thinking budget level string to a token count
func getThinkingBudgetFromLevel(level string) int {
	switch strings.ToLower(level) {
	case "none":
		return thinkingBudgetNone
	case "low":
		return thinkingBudgetLow
	case "medium":
		return thinkingBudgetMedium
	case "high":
		return thinkingBudgetHigh
	default:
		return thinkingBudgetLow // Default to low if invalid level
	}
}

// Helper function to parse an integer environment variable with a default
func parseEnvVarInt(key string, defaultValue int) int {
	if str := os.Getenv(key); str != "" {
		if val, err := strconv.Atoi(str); err == nil {
			return val
		}
		// Log warning directly as logger might not be initialized yet
		fmt.Fprintf(os.Stderr, "[WARN] Invalid integer value for %s: %q. Using default: %d\n", key, str, defaultValue)
	}
	return defaultValue
}

// Helper function to parse a float64 environment variable with a default
func parseEnvVarFloat(key string, defaultValue float64) float64 {
	if str := os.Getenv(key); str != "" {
		if val, err := strconv.ParseFloat(str, 64); err == nil {
			return val
		}
		fmt.Fprintf(os.Stderr, "[WARN] Invalid float value for %s: %q. Using default: %f\n", key, str, defaultValue)
	}
	return defaultValue
}

// Helper function to parse a duration environment variable with a default
func parseEnvVarDuration(key string, defaultValue time.Duration) time.Duration {
	if str := os.Getenv(key); str != "" {
		if val, err := time.ParseDuration(str); err == nil {
			return val
		}
		fmt.Fprintf(os.Stderr, "[WARN] Invalid duration value for %s: %q. Using default: %s\n", key, str, defaultValue.String())
	}
	return defaultValue
}

// Helper function to parse a boolean environment variable with a default
func parseEnvVarBool(key string, defaultValue bool) bool {
	if str := os.Getenv(key); str != "" {
		if val, err := strconv.ParseBool(str); err == nil {
			return val
		}
		fmt.Fprintf(os.Stderr, "[WARN] Invalid boolean value for %s: %q. Using default: %t\n", key, str, defaultValue)
	}
	return defaultValue
}

// NewConfig creates a new configuration from environment variables
func NewConfig() (*Config, error) {
	// No longer validating default model at startup - will be checked when needed
	// This allows for new models not in our hardcoded list
	// Get Gemini API key - required
	geminiAPIKey := os.Getenv("GEMINI_API_KEY")
	if geminiAPIKey == "" {
		return nil, errors.New("GEMINI_API_KEY environment variable is required")
	}

	// Get Gemini model - optional with default
	geminiModel := os.Getenv("GEMINI_MODEL")
	if geminiModel == "" {
		geminiModel = defaultGeminiModel // Default model if not specified
	}
	// Note: We no longer validate the model here to allow for new models
	// and preview versions not in our hardcoded list

	// Get Gemini search model - optional with default
	geminiSearchModel := os.Getenv("GEMINI_SEARCH_MODEL")
	if geminiSearchModel == "" {
		geminiSearchModel = defaultGeminiSearchModel // Default search model if not specified
	}
	// Note: We also don't validate the search model here

	// Get Gemini system prompt - optional with default
	geminiSystemPrompt := os.Getenv("GEMINI_SYSTEM_PROMPT")
	if geminiSystemPrompt == "" {
		geminiSystemPrompt = defaultGeminiSystemPrompt // Default system prompt if not specified
	}

	// Get Gemini search system prompt - optional with default
	geminiSearchSystemPrompt := os.Getenv("GEMINI_SEARCH_SYSTEM_PROMPT")
	if geminiSearchSystemPrompt == "" {
		geminiSearchSystemPrompt = defaultGeminiSearchSystemPrompt // Default search system prompt if not specified
	}

	// Use helper functions to parse environment variables
	timeout := parseEnvVarDuration("GEMINI_TIMEOUT", 90*time.Second)
	maxRetries := parseEnvVarInt("GEMINI_MAX_RETRIES", 2)
	initialBackoff := parseEnvVarDuration("GEMINI_INITIAL_BACKOFF", 1*time.Second)
	maxBackoff := parseEnvVarDuration("GEMINI_MAX_BACKOFF", 10*time.Second)

	// Set default temperature or override with environment variable
	geminiTemperature := parseEnvVarFloat("GEMINI_TEMPERATURE", defaultGeminiTemperature)
	// Specific validation for temperature range, as it's a critical parameter
	if geminiTemperature < 0.0 || geminiTemperature > 1.0 {
		return nil, fmt.Errorf("GEMINI_TEMPERATURE must be between 0.0 and 1.0, got %v", geminiTemperature)
	}

	// File handling settings
	maxFileSize := int64(parseEnvVarInt("GEMINI_MAX_FILE_SIZE", int(defaultMaxFileSize)))
	if maxFileSize <= 0 {
		fmt.Fprintf(os.Stderr, "[WARN] GEMINI_MAX_FILE_SIZE must be positive. Using default: %d\n", defaultMaxFileSize)
		maxFileSize = defaultMaxFileSize
	}

	var allowedFileTypes []string
	if typesStr := os.Getenv("GEMINI_ALLOWED_FILE_TYPES"); typesStr != "" {
		parts := strings.Split(typesStr, ",")
		for _, p := range parts {
			if trimmed := strings.TrimSpace(p); trimmed != "" {
				allowedFileTypes = append(allowedFileTypes, trimmed)
			}
		}
	}
	if len(allowedFileTypes) == 0 {
		allowedFileTypes = []string{
			"text/plain", "text/javascript", "text/typescript",
			"text/markdown", "text/html", "text/css",
			"application/json", "text/yaml", "application/octet-stream",
		}
	}

	// Cache settings
	enableCaching := parseEnvVarBool("GEMINI_ENABLE_CACHING", defaultEnableCaching)
	defaultCacheTTL := parseEnvVarDuration("GEMINI_DEFAULT_CACHE_TTL", defaultDefaultCacheTTL)
	if defaultCacheTTL <= 0 {
		fmt.Fprintf(os.Stderr, "[WARN] GEMINI_DEFAULT_CACHE_TTL must be positive. Using default: %s\n", defaultDefaultCacheTTL.String())
		defaultCacheTTL = defaultDefaultCacheTTL
	}

	// Thinking settings
	enableThinking := parseEnvVarBool("GEMINI_ENABLE_THINKING", defaultEnableThinking)

	// Set thinking budget level from environment variable or use default
	thinkingBudgetLevel := defaultThinkingBudgetLevel
	if levelStr := os.Getenv("GEMINI_THINKING_BUDGET_LEVEL"); levelStr != "" {
		level := strings.ToLower(levelStr)
		if level == "none" || level == "low" || level == "medium" || level == "high" {
			thinkingBudgetLevel = level
		} else {
			fmt.Fprintf(os.Stderr, "[WARN] Invalid GEMINI_THINKING_BUDGET_LEVEL value: %q. Using default: %q\n",
				levelStr, defaultThinkingBudgetLevel)
		}
	}

	// Set thinking budget from environment variable or derive from level
	thinkingBudget := getThinkingBudgetFromLevel(thinkingBudgetLevel)
	// If GEMINI_THINKING_BUDGET is set, it overrides the level-derived value.
	// The helper will use the level-derived budget as a fallback if parsing fails.
	thinkingBudget = parseEnvVarInt("GEMINI_THINKING_BUDGET", thinkingBudget)

	// HTTP transport settings
	enableHTTP := parseEnvVarBool("GEMINI_ENABLE_HTTP", defaultEnableHTTP)
	httpAddress := os.Getenv("GEMINI_HTTP_ADDRESS")
	if httpAddress == "" {
		httpAddress = defaultHTTPAddress
	}
	httpPath := os.Getenv("GEMINI_HTTP_PATH")
	if httpPath == "" {
		httpPath = defaultHTTPPath
	}
	httpStateless := parseEnvVarBool("GEMINI_HTTP_STATELESS", defaultHTTPStateless)
	httpHeartbeat := parseEnvVarDuration("GEMINI_HTTP_HEARTBEAT", defaultHTTPHeartbeat)
	if httpHeartbeat < 0 {
		fmt.Fprintf(os.Stderr, "[WARN] GEMINI_HTTP_HEARTBEAT must be non-negative. Using default: %s\n", defaultHTTPHeartbeat.String())
		httpHeartbeat = defaultHTTPHeartbeat
	}
	httpCORSEnabled := parseEnvVarBool("GEMINI_HTTP_CORS_ENABLED", defaultHTTPCORSEnabled)
	var httpCORSOrigins []string
	if originsStr := os.Getenv("GEMINI_HTTP_CORS_ORIGINS"); originsStr != "" {
		parts := strings.Split(originsStr, ",")
		for _, p := range parts {
			if trimmed := strings.TrimSpace(p); trimmed != "" {
				httpCORSOrigins = append(httpCORSOrigins, trimmed)
			}
		}
	}
	if len(httpCORSOrigins) == 0 {
		httpCORSOrigins = []string{"*"} // Default allow all origins
	}

	// Authentication settings
	authEnabled := parseEnvVarBool("GEMINI_AUTH_ENABLED", defaultAuthEnabled)
	authSecretKey := os.Getenv("GEMINI_AUTH_SECRET_KEY")

	// If authentication is enabled, require secret key
	if authEnabled && authSecretKey == "" {
		return nil, fmt.Errorf("GEMINI_AUTH_SECRET_KEY is required when GEMINI_AUTH_ENABLED=true")
	}

	// Warn if secret key is too short (for security)
	if authEnabled && len(authSecretKey) < 32 {
		fmt.Fprintf(os.Stderr, "[WARN] GEMINI_AUTH_SECRET_KEY should be at least 32 characters for security\n")
	}

	// Prompt defaults
	projectLanguage := os.Getenv("GEMINI_PROJECT_LANGUAGE")
	if projectLanguage == "" {
		projectLanguage = "go"
	}

	promptDefaultAudience := os.Getenv("GEMINI_PROMPT_DEFAULT_AUDIENCE")
	if promptDefaultAudience == "" {
		promptDefaultAudience = "intermediate"
	}

	promptDefaultFocus := os.Getenv("GEMINI_PROMPT_DEFAULT_FOCUS")
	if promptDefaultFocus == "" {
		promptDefaultFocus = "general"
	}

	promptDefaultSeverity := os.Getenv("GEMINI_PROMPT_DEFAULT_SEVERITY")
	if promptDefaultSeverity == "" {
		promptDefaultSeverity = "warning"
	}

	promptDefaultDocFormat := os.Getenv("GEMINI_PROMPT_DEFAULT_DOC_FORMAT")
	if promptDefaultDocFormat == "" {
		promptDefaultDocFormat = "markdown"
	}

	promptDefaultFramework := os.Getenv("GEMINI_PROMPT_DEFAULT_FRAMEWORK")
	if promptDefaultFramework == "" {
		promptDefaultFramework = "standard"
	}

	promptDefaultCoverage := os.Getenv("GEMINI_PROMPT_DEFAULT_COVERAGE")
	if promptDefaultCoverage == "" {
		promptDefaultCoverage = "comprehensive"
	}

	promptDefaultCompliance := os.Getenv("GEMINI_PROMPT_DEFAULT_COMPLIANCE")
	if promptDefaultCompliance == "" {
		promptDefaultCompliance = "OWASP"
	}

	return &Config{
		GeminiAPIKey:             geminiAPIKey,
		GeminiModel:              geminiModel,
		GeminiSearchModel:        geminiSearchModel, // Assign the read value
		GeminiSystemPrompt:       geminiSystemPrompt,
		GeminiSearchSystemPrompt: geminiSearchSystemPrompt,
		GeminiTemperature:        geminiTemperature,
		HTTPTimeout:              timeout,
		EnableHTTP:               enableHTTP,
		HTTPAddress:              httpAddress,
		HTTPPath:                 httpPath,
		HTTPStateless:            httpStateless,
		HTTPHeartbeat:            httpHeartbeat,
		HTTPCORSEnabled:          httpCORSEnabled,
		HTTPCORSOrigins:          httpCORSOrigins,
		AuthEnabled:              authEnabled,
		AuthSecretKey:            authSecretKey,
		MaxRetries:               maxRetries,
		InitialBackoff:           initialBackoff,
		MaxBackoff:               maxBackoff,
		MaxFileSize:              maxFileSize,
		AllowedFileTypes:         allowedFileTypes,
		EnableCaching:            enableCaching,
		DefaultCacheTTL:          defaultCacheTTL,
		EnableThinking:           enableThinking,
		ThinkingBudget:           thinkingBudget,
		ThinkingBudgetLevel:      thinkingBudgetLevel,
		ProjectLanguage:          projectLanguage,
		PromptDefaultAudience:    promptDefaultAudience,
		PromptDefaultFocus:       promptDefaultFocus,
		PromptDefaultSeverity:    promptDefaultSeverity,
		PromptDefaultDocFormat:   promptDefaultDocFormat,
		PromptDefaultFramework:   promptDefaultFramework,
		PromptDefaultCoverage:    promptDefaultCoverage,
		PromptDefaultCompliance:  promptDefaultCompliance,
	}, nil
}
</file>

</files>


<instruction>
# Project Context: GeminiMCP Server

## Overview
This project is a Go-based MCP (Model Control Protocol) server that acts as a bridge to Google's Gemini API. It's designed as a single, self-contained binary for easy deployment and use with MCP-compatible clients. The server exclusively supports the Gemini 2.5 family of models.

## Architecture
- **Language**: Go (Golang)
- **Main Entrypoint**: `main.go`
- **Configuration**: `config.go` (environment variables with CLI overrides)
- **Core Logic**:
    - `gemini_server.go`: Gemini service implementation.
    - `direct_handlers.go`: Handlers for the MCP tools.
    - `prompt_handlers.go`: Handlers for MCP prompts.
    - `tools.go`: Definitions of the MCP tools.
- **Transport**: Supports `stdio` and `http` (with JWT authentication).
- **Dependencies**:
    - `github.com/mark3labs/mcp-go/mcp`: MCP protocol implementation.
    - `github.com/mark3labs/mcp-go/server`: MCP server implementation.
    - `google.golang.org/genai`: Google Gemini API client.
    - `github.com/joho/godotenv`: for loading `.env` files.

## Development Guidelines
- **Build**: `go build -o ./bin/mcp-gemini .`
- **Testing**: `./run_test.sh`
- **Formatting**: `./run_format.sh`
- **Linting**: `./run_lint.sh`
- **Error Handling**: The server has a "degraded mode" to handle initialization errors gracefully.
- **Logging**: A custom logger is used throughout the application.

## AI Assistant Guidelines
- When adding a new tool, define it in `tools.go`, implement the handler in `direct_handlers.go`, and register it in `setupGeminiServer()` in `main.go`.
- When adding a new prompt, define it in `prompts.go`, implement the handler in `prompt_handlers.go` using the `server.PromptHandlerFunc` type, and register it in `setupGeminiServer()`.
- When modifying configuration, update `config.go` for defaults, `NewConfig()` for parsing, `structs.go` for the `Config` struct, and `main.go` for CLI flags.
- Always use `ResolveModelID()` before making API calls to convert model family IDs to specific version IDs.
- Use the existing logging infrastructure for any new logging.
- Follow the existing code style and patterns.

</instruction>
